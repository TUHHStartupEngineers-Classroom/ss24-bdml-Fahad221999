{
  "hash": "9721ec2a64f8ca8203542a9a8603c356",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Challenge 3 (II) & 4 combined Automated Machine Learning and Performance Measures\"\ndate: \"2024-06-20\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    df_print: paged\n    collapsed: false\n    number_sections: true\n    toc_depth: 3\n    #code_folding: hide\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(h2o)\nlibrary(recipes)\nlibrary(rsample)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nproduct_backorders_tbl <- read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/product_backorders.txt\")\nglimpse(product_backorders_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 19,053\nColumns: 23\n$ sku               <dbl> 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n$ national_inv      <dbl> 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n$ lead_time         <dbl> 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n$ in_transit_qty    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ forecast_3_month  <dbl> 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n$ forecast_6_month  <dbl> 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n$ forecast_9_month  <dbl> 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n$ sales_1_month     <dbl> 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n$ sales_3_month     <dbl> 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n$ sales_6_month     <dbl> 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n$ sales_9_month     <dbl> 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n$ min_bank          <dbl> 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n$ potential_issue   <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ pieces_past_due   <dbl> 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ perf_6_month_avg  <dbl> 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n$ perf_12_month_avg <dbl> 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n$ local_bo_qty      <dbl> 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n$ deck_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ oe_constraint     <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ ppap_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n$ stop_auto_buy     <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ rev_stop          <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ went_on_backorder <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_obj <- recipe(went_on_backorder ~ ., data = product_backorders_tbl) %>%\n  step_zv(all_predictors()) %>%\n  step_mutate_at(potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, fn = as.factor) %>%\n  prep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nsplit_obj <- initial_split(product_backorders_tbl, prop = 0.85)\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl <- testing(split_obj)\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\RtmpotJ9VK\\file17fc6018f3/h2o_Lenovo_started_from_r.out\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\RtmpotJ9VK\\file17fc1fcc3462/h2o_Lenovo_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         4 seconds 81 milliseconds \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    6 months and 5 days \n    H2O cluster name:           H2O_started_from_R_Lenovo_pgz902 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.74 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.0 (2023-04-21 ucrt) \n```\n\n\n:::\n\n```{.r .cell-code}\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5 \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n11:22:50.406: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n11:22:50.432: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(automl_models_h2o)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"S4\"\n```\n\n\n:::\n\n```{.r .cell-code}\nslotNames(automl_models_h2o)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n[5] \"modeling_steps\" \"training_info\" \n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                 model_id       auc   logloss\n1 StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 0.9561234 0.1719370\n2    StackedEnsemble_AllModels_3_AutoML_1_20240626_112250 0.9560200 0.1708690\n3    StackedEnsemble_AllModels_2_AutoML_1_20240626_112250 0.9558390 0.1710644\n4    StackedEnsemble_AllModels_1_AutoML_1_20240626_112250 0.9538012 0.1738048\n5 StackedEnsemble_BestOfFamily_2_AutoML_1_20240626_112250 0.9530344 0.1764172\n6                          GBM_4_AutoML_1_20240626_112250 0.9520364 0.1791385\n      aucpr mean_per_class_error      rmse        mse\n1 0.7587667            0.1479800 0.2289585 0.05242201\n2 0.7665524            0.1492160 0.2278504 0.05191579\n3 0.7655005            0.1487976 0.2280735 0.05201754\n4 0.7587346            0.1506517 0.2294995 0.05267003\n5 0.7464036            0.1331207 0.2313708 0.05353243\n6 0.7409863            0.1284338 0.2332589 0.05440969\n\n[23 rows x 7 columns] \n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_models_h2o@leader\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Details:\n==============\n\nH2OBinomialModel: stackedensemble\nModel ID:  StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 \nModel Summary for Stacked Ensemble: \n                                         key            value\n1                          Stacking strategy cross_validation\n2       Number of base models (used / total)              2/5\n3           # GBM base models (used / total)              1/1\n4           # DRF base models (used / total)              1/2\n5  # DeepLearning base models (used / total)              0/1\n6           # GLM base models (used / total)              0/1\n7                      Metalearner algorithm              GLM\n8         Metalearner fold assignment scheme           Random\n9                         Metalearner nfolds                5\n10                   Metalearner fold_column               NA\n11        Custom metalearner hyperparameters             None\n\n\nH2OBinomialMetrics: stackedensemble\n** Reported on training data. **\n\nMSE:  0.01932387\nRMSE:  0.1390103\nLogLoss:  0.07927591\nMean Per-Class Error:  0.05012764\nAUC:  0.9950306\nAUCPR:  0.9716874\nGini:  0.9900613\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         No  Yes    Error        Rate\nNo     8700  103 0.011701   =103/8803\nYes     106 1091 0.088555   =106/1197\nTotals 8806 1194 0.020900  =209/10000\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold       value idx\n1                       max f1  0.424060    0.912589 173\n2                       max f2  0.319633    0.922211 202\n3                 max f0point5  0.572323    0.936852 134\n4                 max accuracy  0.441319    0.979200 168\n5                max precision  0.993213    1.000000   0\n6                   max recall  0.040059    1.000000 324\n7              max specificity  0.993213    1.000000   0\n8             max absolute_mcc  0.424060    0.900721 173\n9   max min_per_class_accuracy  0.241678    0.962406 227\n10 max mean_per_class_accuracy  0.231269    0.963242 231\n11                     max tns  0.993213 8803.000000   0\n12                     max fns  0.993213 1194.000000   0\n13                     max fps  0.000338 8803.000000 399\n14                     max tps  0.040059 1197.000000 324\n15                     max tnr  0.993213    1.000000   0\n16                     max fnr  0.993213    0.997494   0\n17                     max fpr  0.000338    1.000000 399\n18                     max tpr  0.040059    1.000000 324\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: stackedensemble\n** Reported on validation data. **\n\nMSE:  0.04593089\nRMSE:  0.2143149\nLogLoss:  0.1549916\nMean Per-Class Error:  0.152176\nAUC:  0.9575806\nAUCPR:  0.7697308\nGini:  0.9151613\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         No Yes    Error       Rate\nNo     2040  89 0.041804   =89/2129\nYes      68 191 0.262548    =68/259\nTotals 2108 280 0.065745  =157/2388\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold       value idx\n1                       max f1  0.396232    0.708720 160\n2                       max f2  0.166073    0.776732 240\n3                 max f0point5  0.686776    0.744337  91\n4                 max accuracy  0.522239    0.939698 132\n5                max precision  0.987921    1.000000   0\n6                   max recall  0.013005    1.000000 361\n7              max specificity  0.987921    1.000000   0\n8             max absolute_mcc  0.522239    0.675954 132\n9   max min_per_class_accuracy  0.166073    0.891892 240\n10 max mean_per_class_accuracy  0.132816    0.897120 254\n11                     max tns  0.987921 2129.000000   0\n12                     max fns  0.987921  258.000000   0\n13                     max fps  0.000173 2129.000000 399\n14                     max tps  0.013005  259.000000 361\n15                     max tnr  0.987921    1.000000   0\n16                     max fnr  0.987921    0.996139   0\n17                     max fpr  0.000173    1.000000 399\n18                     max tpr  0.013005    1.000000 361\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: stackedensemble\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.05090027\nRMSE:  0.2256109\nLogLoss:  0.1725378\nMean Per-Class Error:  0.1609015\nAUC:  0.9499098\nAUCPR:  0.7460786\nGini:  0.8998197\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n          No  Yes    Error         Rate\nNo     11600  553 0.045503   =553/12153\nYes      457 1197 0.276300    =457/1654\nTotals 12057 1750 0.073151  =1010/13807\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold        value idx\n1                       max f1  0.350194     0.703290 201\n2                       max f2  0.164018     0.777717 265\n3                 max f0point5  0.562723     0.738517 137\n4                 max accuracy  0.540548     0.932498 144\n5                max precision  0.988893     1.000000   0\n6                   max recall  0.000456     1.000000 399\n7              max specificity  0.988893     1.000000   0\n8             max absolute_mcc  0.411648     0.662743 183\n9   max min_per_class_accuracy  0.114396     0.883979 286\n10 max mean_per_class_accuracy  0.105893     0.886304 290\n11                     max tns  0.988893 12153.000000   0\n12                     max fns  0.988893  1650.000000   0\n13                     max fps  0.000456 12153.000000 399\n14                     max tps  0.000456  1654.000000 399\n15                     max tnr  0.988893     1.000000   0\n16                     max fnr  0.988893     0.997582   0\n17                     max fpr  0.000456     1.000000 399\n18                     max tpr  0.000456     1.000000 399\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n                mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\naccuracy    0.928743  0.007437   0.936990   0.918571   0.934186   0.929623\nauc         0.950190  0.002653   0.954707   0.950147   0.948655   0.949449\nerr         0.071257  0.007437   0.063010   0.081429   0.065814   0.070377\nerr_count 197.000000 23.323807 170.000000 228.000000 180.000000 196.000000\nf0point5    0.700801  0.027873   0.722807   0.659824   0.730230   0.692137\n          cv_5_valid\naccuracy    0.924346\nauc         0.947989\nerr         0.075654\nerr_count 211.000000\nf0point5    0.699009\n\n---\n                        mean        sd cv_1_valid cv_2_valid cv_3_valid\nprecision           0.693693  0.043396   0.733096   0.633803   0.738710\nr2                  0.517460  0.006031   0.518965   0.506979   0.518830\nrecall              0.737920  0.045015   0.684385   0.789474   0.698171\nresidual_deviance 952.444000 54.130043 870.332340 988.683350 946.922700\nrmse                0.225448  0.005405   0.218355   0.229921   0.225355\nspecificity         0.954526  0.013208   0.968711   0.936534   0.966348\n                  cv_4_valid  cv_5_valid\nprecision           0.675676    0.687180\nr2                  0.522391    0.520138\nrecall              0.766871    0.750700\nresidual_deviance 944.129760 1012.151800\nrmse                0.222177    0.231433\nspecificity         0.951200    0.949836\n```\n\n\n:::\n\n```{.r .cell-code}\ntypeof(automl_models_h2o@leader)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"S4\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- h2o.predict(automl_models_h2o@leader, newdata = as.h2o(test_tbl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions_tbl <- \n  predictions %>% \n    as_tibble()\n#h2o.saveModel(automl_models_h2o@leader, path = \"./04_perf_meas_files/\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_tbl %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,858\nColumns: 3\n$ predict <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Yes,…\n$ No      <dbl> 0.35334247, 0.36027327, 0.16122040, 0.18614910, 0.15906951, 0.…\n$ Yes     <dbl> 0.64665753, 0.63972673, 0.83877960, 0.81385090, 0.84093049, 0.…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard %>% \n              as_tibble() %>% \n              select(-c(mean_per_class_error, rmse, mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 23 × 4\n   model_id                                                  auc logloss aucpr\n   <chr>                                                   <dbl>   <dbl> <dbl>\n 1 StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 0.956   0.172 0.759\n 2 StackedEnsemble_AllModels_3_AutoML_1_20240626_112250    0.956   0.171 0.767\n 3 StackedEnsemble_AllModels_2_AutoML_1_20240626_112250    0.956   0.171 0.766\n 4 StackedEnsemble_AllModels_1_AutoML_1_20240626_112250    0.954   0.174 0.759\n 5 StackedEnsemble_BestOfFamily_2_AutoML_1_20240626_112250 0.953   0.176 0.746\n 6 GBM_4_AutoML_1_20240626_112250                          0.952   0.179 0.741\n 7 GBM_grid_1_AutoML_1_20240626_112250_model_1             0.951   0.178 0.744\n 8 GBM_grid_1_AutoML_1_20240626_112250_model_2             0.951   0.179 0.755\n 9 GBM_1_AutoML_1_20240626_112250                          0.950   0.179 0.753\n10 StackedEnsemble_BestOfFamily_1_AutoML_1_20240626_112250 0.950   0.179 0.753\n# ℹ 13 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n\n    # Setup inputs\n    # adjust input so that all formats are working\n    order_by <- tolower(order_by[[1]])\n\n    leaderboard_tbl <- h2o_leaderboard %>%\n        as_tibble() %>%\n        select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n        rownames_to_column(var = \"rowname\") %>%\n        mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n\n    # Transformation\n    if (order_by == \"auc\") {\n\n        data_transformed_tbl <- leaderboard_tbl %>%\n            slice(1:n_max) %>%\n            mutate(\n                model_id   = as_factor(model_id) %>% reorder(auc),\n                model_type = as.factor(model_type)\n            ) %>%\n                pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else if (order_by == \"logloss\") {\n\n        data_transformed_tbl <- leaderboard_tbl %>%\n            slice(1:n_max) %>%\n            mutate(\n                model_id   = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n                model_type = as.factor(model_type)\n            ) %>%\n            pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else {\n        # If nothing is supplied\n        stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n    }\n\n    # Visualization\n    g <- data_transformed_tbl %>%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = size) +\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n             y = \"Model Postion, Model ID\", x = \"\")\n\n    if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), \n                                             hjust = \"inward\"))\n\n    return(g)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard %>% plot_h2o_leaderboard()\n```\n\n::: {.cell-output-display}\n![](Chapter_3b-_and_4_Challenge_fahad_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nI was unable to build the page because the program have some problem with an h2o function. So I saved the results and loaded them. The used code is commented out for a better understanding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#h2o.init()\n#deeplearning_h2o <- \n#h2o.loadModel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/DeepLearning_1_AutoML_3_20220614_234925\")\n#deeplearning_h2o@allparameters\n\n#Deeplearning_grid_01 <- h2o.grid()\n \n#     # See help page for available algos\n    #algorithm = \"deeplearning\"\n#     \n#     # I just use the same as the object\n    #grid_id = \"Deaplearning_grid_01\"\n#     \n#     # The following is for ?h2o.deeplearning()\n#     # predictor and response variables\n     #x = x,\n     #y = y,\n#     \n#     # training and validation frame and crossfold validation\n#     training_frame   = train_h2o,\n#     validation_frame = valid_h2o,\n#     nfolds = 5,\n#     \n#     # Hyperparamters: Use deeplearning_h2o@allparameters to see all\n#     hyper_params = list(\n#         # Use some combinations (the first one was the original)\n#         hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n#         epochs = c(10, 50, 100)\n#     )\n# )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# <- h2o.getModel(\"Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 %>%h2o.saveModel(path = \"04_Modeling/Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 <- h2o.loadModel(\"04_Modeling/Deaplearning_grid_01_model_3/Deaplearning_grid_01_model_3\")\n# performance_h2o <- h2o.performance(Deeplearning_grid_01_model_3, newdata = as.h2o(test_tbl))\n# \n# performance_tbl <- performance_h2o %>%\n#     h2o.metric() %>%\n#     as.tibble()\n# \n# theme_new <- theme(\n#       legend.position  = \"bottom\",\n#       panel.background = element_rect(fill   = \"transparent\"),\n#       panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n#       panel.grid.major = element_line(color = \"grey\", size = 0.333)\n#       ) \n #saveRDS(performance_tbl, file = \"performance_tbl.rds\")\n\n#performance_tbl <- readRDS(\"performance_tbl.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#performance_tbl %>%\n    #filter(f1 == max(f1))\n\n#performance_tbl %>%\n    #ggplot(aes(x = threshold)) +\n    #geom_line(aes(y = precision), color = \"blue\", size = 1) +\n    #geom_line(aes(y = recall), color = \"red\", size = 1) +\n    \n    # Insert line where precision and recall are harmonically optimized\n    #geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n    #labs(title = \"Precision vs Recall\", y = \"value\") +\n    #theme_new\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-15-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#p1 <- performance_tbl %>%\n  #ggplot(aes(fpr, tpr)) +\n    #geom_line(size = 1) +\n    \n    # just for demonstration purposes\n    #geom_abline(color = \"red\", linetype = \"dotted\") +\n    \n    #theme_new +\n    #theme(\n      #legend.direction = \"vertical\",\n      #) +\n    #labs(\n        #title = \"ROC Plot\"\n        #subtitle = \"Performance of 3 Top Performing Models\"\n   # )\n#p1\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-16-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#p2 <- performance_tbl %>%\n  #ggplot(aes(recall, precision)) +\n    #geom_line(size = 1) +\n    #theme_new + \n    #theme(\n      #legend.direction = \"vertical\",\n      #) +\n    #labs(\n        #title = \"Precision vs Recall Plot\"\n        #subtitle = \"Performance of 3 Top Performing Models\"\n    #)\n#p2\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-17-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nranked_predictions_tbl <- predictions_tbl %>%\n    bind_cols(test_tbl) %>%\n    select(predict:Yes, went_on_backorder) %>%\n    # Sorting from highest to lowest class probability\n    arrange(desc(Yes))\n\ncalculated_gain_lift_tbl <- ranked_predictions_tbl %>%\n    mutate(ntile = ntile(Yes, n = 10)) %>%\n    group_by(ntile) %>%\n    summarise(\n        cases = n(),\n        responses = sum(went_on_backorder == \"Yes\")\n    ) %>%\n    arrange(desc(ntile)) %>%\n    \n    # Add group numbers (opposite of ntile)\n    mutate(group = row_number()) %>%\n    select(group, cases, responses) %>%\n    \n    # Calculations\n    mutate(\n        cumulative_responses = cumsum(responses),\n        pct_responses        = responses / sum(responses),\n        gain                 = cumsum(pct_responses),\n        cumulative_pct_cases = cumsum(cases) / sum(cases),\n        lift                 = gain / cumulative_pct_cases,\n        gain_baseline        = cumulative_pct_cases,\n        lift_baseline        = gain_baseline / cumulative_pct_cases\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#gain_lift_tbl <- performance_h2o %>%\n    #h2o.gainsLift() %>%\n    #as.tibble()\n\n#gain_transformed_tbl <- gain_lift_tbl %>% \n    #select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n    #select(-contains(\"lift\")) %>%\n    #mutate(baseline = cumulative_data_fraction) %>%\n    #rename(gain     = cumulative_capture_rate) %>%\n    # prepare the data for the plotting (for the color and group aesthetics)\n    #pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\n#p3 <- gain_transformed_tbl %>%\n    #ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n    #geom_line(size = 1.5) +\n    #labs(\n        #title = \"Gain Chart\",\n        #x = \"Cumulative Data Fraction\",\n        #y = \"Gain\"\n    #) +\n    #theme_new\n#p3\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-19-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#lift_transformed_tbl <- gain_lift_tbl %>% \n#    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n#    select(-contains(\"capture\")) %>%\n#    mutate(baseline = 1) %>%\n#    rename(lift = cumulative_lift) %>%\n#    pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n#\n#p4 <- lift_transformed_tbl %>%\n#    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n#    geom_line(size = 1.5) +\n#    labs(\n#        title = \"Lift Chart\",\n#        x = \"Cumulative Data Fraction\",\n#        y = \"Lift\"\n#    ) +\n#    theme_new\n#p4\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-20-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\nlibrary(glue)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine using cowplot\n   # \n#    # cowplot::get_legend extracts a legend from a ggplot object\n  #  p_legend <- get_legend(p1)\n#    # Remove legend from p1\n #   p1 <- p1 + theme(legend.position = \"none\")\n    \n    # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n#    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n #   p\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-21-1.png\")\n```\n\n::: {.cell-output-display}\n![](E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}