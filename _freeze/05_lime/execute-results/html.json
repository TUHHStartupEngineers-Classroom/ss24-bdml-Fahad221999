{
  "hash": "5e4d78e2c222212e81a7a8287fe5ab79",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Challenge 5- LIME\"\ndate: \"2024-21-06\"\noutput:\n  html_document:\n    toc: yes\n    toc_float: yes\n    df_print: paged\n    collapsed: no\n    number_sections: yes\n    toc_depth: 3\n  pdf_document:\n    toc: yes\n    toc_depth: '3'\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(h2o)\nlibrary(recipes)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(lime)\nlibrary(rsample)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_hr_data_readable <- function(data, definitions_tbl) {\n\n    definitions_list <- definitions_tbl %>%\n        fill(...1, .direction = \"down\") %>%\n        filter(!is.na(...2)) %>%\n        separate(...2, into = c(\"key\", \"value\"), sep = \" '\", remove = TRUE) %>%\n        rename(column_name = ...1) %>%\n        mutate(key = as.numeric(key)) %>%\n        mutate(value = value %>% str_replace(pattern = \"'\", replacement = \"\")) %>%\n        split(.$column_name) %>%\n        map(~ select(., -column_name)) %>%\n        map(~ mutate(., value = as_factor(value))) \n    \n    for (i in seq_along(definitions_list)) {\n        list_name <- names(definitions_list)[i]\n        colnames(definitions_list[[i]]) <- c(list_name, paste0(list_name, \"_value\"))\n    }\n    \n    data_merged_tbl <- list(HR_Data = data) %>%\n        append(definitions_list, after = 1) %>%\n        reduce(left_join) %>%\n        select(-one_of(names(definitions_list))) %>%\n        set_names(str_replace_all(names(.), pattern = \"_value\", \n                                            replacement = \"\")) %>%\n        select(sort(names(.))) %>%\n        mutate_if(is.character, as.factor) %>%\n        mutate(\n            BusinessTravel = BusinessTravel %>% fct_relevel(\"Non-Travel\", \n                                                            \"Travel_Rarely\", \n                                                            \"Travel_Frequently\"),\n            MaritalStatus  = MaritalStatus %>% fct_relevel(\"Single\", \n                                                           \"Married\", \n                                                           \"Divorced\")\n        )\n    \n    return(data_merged_tbl)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nemployee_attrition_tbl <- read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/03_ml_aut_files/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\")\ndefinitions_raw_tbl    <- read_excel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/03_ml_aut_files/data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\nrecipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%\n                step_zv(all_predictors()) %>%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n                prep()\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\Rtmp6PurmV\\file2d507d7a45b9/h2o_Lenovo_started_from_r.out\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\Rtmp6PurmV\\file2d506c827f47/h2o_Lenovo_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         2 seconds 771 milliseconds \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    6 months and 5 days \n    H2O cluster name:           H2O_started_from_R_Lenovo_drs368 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.74 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.0 (2023-04-21 ucrt) \n```\n\n\n:::\n\n```{.r .cell-code}\n#automl_leader <- h2o.loadModel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/StackedEnsemble_BestOfFamily_2_AutoML_1_20220612_170511\")\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set the target and predictors\ny <- \"Attrition\"\nx <- setdiff(names(train_h2o), y)\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5 \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n11:20:43.696: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n11:20:43.725: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_leader <- automl_models_h2o@leader\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexplainer <- train_tbl %>%\n    select(-Attrition) %>%\n    lime(\n        model           = automl_leader,\n        bin_continuous  = TRUE,\n        n_bins          = 4,\n        quantile_bins   = TRUE\n    )\n\nexplainer\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$model\nModel Details:\n==============\n\nH2OBinomialModel: stackedensemble\nModel ID:  StackedEnsemble_AllModels_2_AutoML_1_20240626_112043 \nModel Summary for Stacked Ensemble: \n                                         key            value\n1                          Stacking strategy cross_validation\n2       Number of base models (used / total)              5/9\n3           # GBM base models (used / total)              3/5\n4  # DeepLearning base models (used / total)              1/1\n5           # GLM base models (used / total)              1/1\n6           # DRF base models (used / total)              0/2\n7                      Metalearner algorithm              GLM\n8         Metalearner fold assignment scheme           Random\n9                         Metalearner nfolds                5\n10                   Metalearner fold_column               NA\n11        Custom metalearner hyperparameters             None\n\n\nH2OBinomialMetrics: stackedensemble\n** Reported on training data. **\n\nMSE:  0.05375346\nRMSE:  0.2318479\nLogLoss:  0.2027985\nMean Per-Class Error:  0.1373576\nAUC:  0.9387487\nAUCPR:  0.8592039\nGini:  0.8774974\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error      Rate\nNo     888  21 0.023102   =21/909\nYes     39 116 0.251613   =39/155\nTotals 927 137 0.056391  =60/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.376327   0.794521 114\n2                       max f2  0.230400   0.803140 164\n3                 max f0point5  0.551058   0.856874  74\n4                 max accuracy  0.420139   0.944549 105\n5                max precision  0.975793   1.000000   0\n6                   max recall  0.006598   1.000000 388\n7              max specificity  0.975793   1.000000   0\n8             max absolute_mcc  0.420139   0.764506 105\n9   max min_per_class_accuracy  0.185924   0.878988 183\n10 max mean_per_class_accuracy  0.230400   0.887778 164\n11                     max tns  0.975793 909.000000   0\n12                     max fns  0.975793 154.000000   0\n13                     max fps  0.000731 909.000000 399\n14                     max tps  0.006598 155.000000 388\n15                     max tnr  0.975793   1.000000   0\n16                     max fnr  0.975793   0.993548   0\n17                     max fpr  0.000731   1.000000 399\n18                     max tpr  0.006598   1.000000 388\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: stackedensemble\n** Reported on validation data. **\n\nMSE:  0.1008602\nRMSE:  0.317585\nLogLoss:  0.3336465\nMean Per-Class Error:  0.1987111\nAUC:  0.868063\nAUCPR:  0.7193464\nGini:  0.736126\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error     Rate\nNo     135  12 0.081633  =12/147\nYes     12  26 0.315789   =12/38\nTotals 147  38 0.129730  =24/185\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.337626   0.684211  37\n2                       max f2  0.200157   0.714286  57\n3                 max f0point5  0.474533   0.708955  23\n4                 max accuracy  0.474533   0.870270  23\n5                max precision  0.940023   1.000000   0\n6                   max recall  0.017752   1.000000 145\n7              max specificity  0.940023   1.000000   0\n8             max absolute_mcc  0.337626   0.602578  37\n9   max min_per_class_accuracy  0.200157   0.789474  57\n10 max mean_per_class_accuracy  0.269144   0.807197  45\n11                     max tns  0.940023 147.000000   0\n12                     max fns  0.940023  37.000000   0\n13                     max fps  0.001591 147.000000 184\n14                     max tps  0.017752  38.000000 145\n15                     max tnr  0.940023   1.000000   0\n16                     max fnr  0.940023   0.973684   0\n17                     max fpr  0.001591   1.000000 184\n18                     max tpr  0.017752   1.000000 145\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: stackedensemble\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.08590584\nRMSE:  0.293097\nLogLoss:  0.3021878\nMean Per-Class Error:  0.2155967\nAUC:  0.8392562\nAUCPR:  0.6039713\nGini:  0.6785124\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     822  87 0.095710    =87/909\nYes     52 103 0.335484    =52/155\nTotals 874 190 0.130639  =139/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.253182   0.597101 144\n2                       max f2  0.165232   0.642002 201\n3                 max f0point5  0.396519   0.650741  90\n4                 max accuracy  0.396519   0.896617  90\n5                max precision  0.967443   1.000000   0\n6                   max recall  0.000598   1.000000 399\n7              max specificity  0.967443   1.000000   0\n8             max absolute_mcc  0.396519   0.540769  90\n9   max min_per_class_accuracy  0.146528   0.767742 216\n10 max mean_per_class_accuracy  0.244599   0.786454 149\n11                     max tns  0.967443 909.000000   0\n12                     max fns  0.967443 154.000000   0\n13                     max fps  0.000598 909.000000 399\n14                     max tps  0.000598 155.000000 399\n15                     max tnr  0.967443   1.000000   0\n16                     max fnr  0.967443   0.993548   0\n17                     max fpr  0.000598   1.000000 399\n18                     max tpr  0.000598   1.000000 399\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n               mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\naccuracy   0.897478 0.013204   0.876238   0.905983   0.901554   0.909548\nauc        0.837242 0.065571   0.800792   0.878349   0.852814   0.909991\nerr        0.102522 0.013204   0.123762   0.094017   0.098446   0.090452\nerr_count 21.800000 3.271085  25.000000  22.000000  19.000000  18.000000\nf0point5   0.632989 0.096357   0.590551   0.754717   0.666667   0.657143\n          cv_5_valid\naccuracy    0.894068\nauc         0.744265\nerr         0.105932\nerr_count  25.000000\nf0point5    0.495868\n\n---\n                        mean        sd cv_1_valid cv_2_valid cv_3_valid\nprecision           0.640836  0.097618   0.625000   0.761905   0.695652\nr2                  0.296269  0.099698   0.244036   0.388037   0.302603\nrecall              0.622885  0.162566   0.483871   0.727273   0.571429\nresidual_deviance 127.125380 20.503176 134.964460 146.976350 113.411110\nrmse                0.292640  0.018314   0.313385   0.305668   0.294106\nspecificity         0.942809  0.014539   0.947368   0.947368   0.957576\n                  cv_4_valid cv_5_valid\nprecision           0.621622   0.500000\nr2                  0.390633   0.156038\nrecall              0.851852   0.480000\nresidual_deviance  98.454940 141.820050\nrmse                0.267321   0.282723\nspecificity         0.918605   0.943128\n\n$preprocess\nfunction (x) \nx\n<bytecode: 0x000001e6c0fee200>\n<environment: 0x000001e6c0ff6518>\n\n$bin_continuous\n[1] TRUE\n\n$n_bins\n[1] 4\n\n$quantile_bins\n[1] TRUE\n\n$use_density\n[1] TRUE\n\n$feature_type\n                     Age           BusinessTravel                DailyRate \n               \"numeric\"                 \"factor\"                \"numeric\" \n              Department         DistanceFromHome                Education \n                \"factor\"                \"numeric\"                 \"factor\" \n          EducationField           EmployeeNumber  EnvironmentSatisfaction \n                \"factor\"                \"numeric\"                 \"factor\" \n                  Gender               HourlyRate           JobInvolvement \n                \"factor\"                \"numeric\"                 \"factor\" \n                JobLevel                  JobRole          JobSatisfaction \n                \"factor\"                 \"factor\"                 \"factor\" \n           MaritalStatus            MonthlyIncome              MonthlyRate \n                \"factor\"                \"numeric\"                \"numeric\" \n      NumCompaniesWorked                 OverTime        PercentSalaryHike \n               \"numeric\"                 \"factor\"                \"numeric\" \n       PerformanceRating RelationshipSatisfaction         StockOptionLevel \n                \"factor\"                 \"factor\"                 \"factor\" \n       TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance \n               \"numeric\"                \"numeric\"                 \"factor\" \n          YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion \n               \"numeric\"                \"numeric\"                \"numeric\" \n    YearsWithCurrManager \n               \"numeric\" \n\n$bin_cuts\n$bin_cuts$Age\n  0%  25%  50%  75% 100% \n  18   30   36   43   60 \n\n$bin_cuts$BusinessTravel\nNULL\n\n$bin_cuts$DailyRate\n  0%  25%  50%  75% 100% \n 102  465  797 1147 1499 \n\n$bin_cuts$Department\nNULL\n\n$bin_cuts$DistanceFromHome\n  0%  25%  50%  75% 100% \n   1    2    7   14   29 \n\n$bin_cuts$Education\nNULL\n\n$bin_cuts$EducationField\nNULL\n\n$bin_cuts$EmployeeNumber\n  0%  25%  50%  75% 100% \n   1  511 1040 1573 2065 \n\n$bin_cuts$EnvironmentSatisfaction\nNULL\n\n$bin_cuts$Gender\nNULL\n\n$bin_cuts$HourlyRate\n  0%  25%  50%  75% 100% \n  30   49   66   83  100 \n\n$bin_cuts$JobInvolvement\nNULL\n\n$bin_cuts$JobLevel\nNULL\n\n$bin_cuts$JobRole\nNULL\n\n$bin_cuts$JobSatisfaction\nNULL\n\n$bin_cuts$MaritalStatus\nNULL\n\n$bin_cuts$MonthlyIncome\n   0%   25%   50%   75%  100% \n 1051  2929  4908  8474 19999 \n\n$bin_cuts$MonthlyRate\n   0%   25%   50%   75%  100% \n 2094  8423 14470 20689 26968 \n\n$bin_cuts$NumCompaniesWorked\n  0%  25%  50%  75% 100% \n   0    1    2    4    9 \n\n$bin_cuts$OverTime\nNULL\n\n$bin_cuts$PercentSalaryHike\n  0%  25%  50%  75% 100% \n  11   12   14   18   25 \n\n$bin_cuts$PerformanceRating\nNULL\n\n$bin_cuts$RelationshipSatisfaction\nNULL\n\n$bin_cuts$StockOptionLevel\nNULL\n\n$bin_cuts$TotalWorkingYears\n  0%  25%  50%  75% 100% \n   0    6   10   15   38 \n\n$bin_cuts$TrainingTimesLastYear\n  0%  25%  50% 100% \n   0    2    3    6 \n\n$bin_cuts$WorkLifeBalance\nNULL\n\n$bin_cuts$YearsAtCompany\n  0%  25%  50%  75% 100% \n   0    3    5    9   37 \n\n$bin_cuts$YearsInCurrentRole\n  0%  25%  50%  75% 100% \n   0    2    3    7   18 \n\n$bin_cuts$YearsSinceLastPromotion\n  0%  50%  75% 100% \n   0    1    2   15 \n\n$bin_cuts$YearsWithCurrManager\n  0%  25%  50%  75% 100% \n   0    2    3    7   17 \n\n\n$feature_distribution\n$feature_distribution$Age\n\n        1         2         3         4 \n0.2602082 0.2834267 0.2217774 0.2345877 \n\n$feature_distribution$BusinessTravel\n\n       Non-Travel     Travel_Rarely Travel_Frequently \n        0.1000801         0.7181745         0.1817454 \n\n$feature_distribution$DailyRate\n\n        1         2         3         4 \n0.2514011 0.2489992 0.2497998 0.2497998 \n\n$feature_distribution$Department\n\n       Human Resources Research & Development                  Sales \n            0.04323459             0.65092074             0.30584468 \n\n$feature_distribution$DistanceFromHome\n\n        1         2         3         4 \n0.2954363 0.2369896 0.2241793 0.2433947 \n\n$feature_distribution$Education\n\nBelow College       College      Bachelor        Master        Doctor \n   0.11689351    0.18895116    0.38510809    0.27461970    0.03442754 \n\n$feature_distribution$EducationField\n\n Human Resources    Life Sciences        Marketing          Medical \n      0.01761409       0.41793435       0.10888711       0.31144916 \n           Other Technical Degree \n      0.05444355       0.08967174 \n\n$feature_distribution$EmployeeNumber\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$EnvironmentSatisfaction\n\n      Low    Medium      High Very High \n0.1913531 0.1961569 0.3018415 0.3106485 \n\n$feature_distribution$Gender\n\n   Female      Male \n0.4123299 0.5876701 \n\n$feature_distribution$HourlyRate\n\n        1         2         3         4 \n0.2618094 0.2473979 0.2449960 0.2457966 \n\n$feature_distribution$JobInvolvement\n\n       Low     Medium       High  Very High \n0.05684548 0.25780624 0.58927142 0.09607686 \n\n$feature_distribution$JobLevel\n\n         1          2          3          4          5 \n0.36829464 0.36509207 0.14651721 0.07526021 0.04483587 \n\n$feature_distribution$JobRole\n\nHealthcare Representative           Human Resources     Laboratory Technician \n               0.08646918                0.03682946                0.18174540 \n                  Manager    Manufacturing Director         Research Director \n               0.06885508                0.09927942                0.05924740 \n       Research Scientist           Sales Executive      Sales Representative \n               0.18654924                0.22337870                0.05764612 \n\n$feature_distribution$JobSatisfaction\n\n      Low    Medium      High Very High \n0.1873499 0.1985588 0.3018415 0.3122498 \n\n$feature_distribution$MaritalStatus\n\n   Single   Married  Divorced \n0.3306645 0.4571657 0.2121697 \n\n$feature_distribution$MonthlyIncome\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$MonthlyRate\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$NumCompaniesWorked\n\n         1          2          3          4 \n0.48118495 0.09927942 0.20496397 0.21457166 \n\n$feature_distribution$OverTime\n\n       No       Yes \n0.7165733 0.2834267 \n\n$feature_distribution$PercentSalaryHike\n\n        1         2         3         4 \n0.2866293 0.2738191 0.2289832 0.2105685 \n\n$feature_distribution$PerformanceRating\n\n        Low        Good   Excellent Outstanding \n  0.0000000   0.0000000   0.8414732   0.1585268 \n\n$feature_distribution$RelationshipSatisfaction\n\n      Low    Medium      High Very High \n0.1889512 0.2161729 0.3018415 0.2930344 \n\n$feature_distribution$StockOptionLevel\n\n         0          1          2          3 \n0.43554844 0.40592474 0.10168135 0.05684548 \n\n$feature_distribution$TotalWorkingYears\n\n        1         2         3         4 \n0.3050440 0.3306645 0.1224980 0.2417934 \n\n$feature_distribution$TrainingTimesLastYear\n\n        1         2         3 \n0.4603683 0.3306645 0.2089672 \n\n$feature_distribution$WorkLifeBalance\n\n       Bad       Good     Better       Best \n0.05204163 0.22497998 0.61889512 0.10408327 \n\n$feature_distribution$YearsAtCompany\n\n        1         2         3         4 \n0.3226581 0.2137710 0.2217774 0.2417934 \n\n$feature_distribution$YearsInCurrentRole\n\n         1          2          3          4 \n0.46757406 0.08726982 0.27542034 0.16973579 \n\n$feature_distribution$YearsSinceLastPromotion\n\n        1         2         3 \n0.6413131 0.1120897 0.2465973 \n\n$feature_distribution$YearsWithCurrManager\n\n         1          2          3          4 \n0.46357086 0.09767814 0.25300240 0.18574860 \n\n\nattr(,\"class\")\n[1] \"data_frame_explainer\" \"explainer\"            \"list\"                \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexplanation <- test_tbl %>%\n    slice(1:20) %>%\n    select(-Attrition) %>%\n    lime::explain(\n    \n        # Pass our explainer object\n        explainer = explainer,\n        # Because it is a binary classification model: 1\n        n_labels   = 1,\n        # number of features to be returned\n        n_features = 8,\n        # number of localized linear models\n        n_permutations = 5000,\n        # Let's start with 1\n        kernel_width   = 1\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\nexplanation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 160 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n 1 classificat… 1     No         0.565    0.345           0.907            0.517\n 2 classificat… 1     No         0.565    0.345           0.907            0.517\n 3 classificat… 1     No         0.565    0.345           0.907            0.517\n 4 classificat… 1     No         0.565    0.345           0.907            0.517\n 5 classificat… 1     No         0.565    0.345           0.907            0.517\n 6 classificat… 1     No         0.565    0.345           0.907            0.517\n 7 classificat… 1     No         0.565    0.345           0.907            0.517\n 8 classificat… 1     No         0.565    0.345           0.907            0.517\n 9 classificat… 2     No         0.777    0.420           0.737            0.620\n10 classificat… 2     No         0.777    0.420           0.737            0.620\n# ℹ 150 more rows\n# ℹ 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#   feature_desc <chr>, data <list>, prediction <list>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexplanation %>% \n  as.tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 160 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n 1 classificat… 1     No         0.565    0.345           0.907            0.517\n 2 classificat… 1     No         0.565    0.345           0.907            0.517\n 3 classificat… 1     No         0.565    0.345           0.907            0.517\n 4 classificat… 1     No         0.565    0.345           0.907            0.517\n 5 classificat… 1     No         0.565    0.345           0.907            0.517\n 6 classificat… 1     No         0.565    0.345           0.907            0.517\n 7 classificat… 1     No         0.565    0.345           0.907            0.517\n 8 classificat… 1     No         0.565    0.345           0.907            0.517\n 9 classificat… 2     No         0.777    0.420           0.737            0.620\n10 classificat… 2     No         0.777    0.420           0.737            0.620\n# ℹ 150 more rows\n# ℹ 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#   feature_desc <chr>, data <list>, prediction <list>\n```\n\n\n:::\n\n```{.r .cell-code}\ncase_1 <- explanation %>%\n    filter(case == 1)\n\ncase_1 %>%\n    plot_features()\n```\n\n::: {.cell-output-display}\n![](05_lime_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncase_1 %>%\n  ggplot(aes(y=feature_desc, x =feature_weight)) +\n  geom_col(aes(fill = feature_weight > 0)) +\n  xlab(\"Weight\") + \n  ylab(\"Feature\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Contradicts\", \"Supports\")) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](05_lime_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexplanation %>%\n  mutate(case = as.double(case)) %>%\n  ggplot(aes(y=feature_desc, x =case, fill = feature_weight)) +\n  geom_tile() +\n  facet_wrap(~label) \n```\n\n::: {.cell-output-display}\n![](05_lime_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}