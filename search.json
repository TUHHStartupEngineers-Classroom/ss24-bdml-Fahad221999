[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "Chapter_3b _and_4_Challenge_fahad.html",
    "href": "Chapter_3b _and_4_Challenge_fahad.html",
    "title": "Challenge 3 (II) & 4 combined Automated Machine Learning and Performance Measures",
    "section": "",
    "text": "library(tidyverse)\nlibrary(GGally)\nlibrary(h2o)\nlibrary(recipes)\nlibrary(rsample)\n\n\nproduct_backorders_tbl &lt;- read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/product_backorders.txt\")\nglimpse(product_backorders_tbl)\n\nRows: 19,053\nColumns: 23\n$ sku               &lt;dbl&gt; 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n$ national_inv      &lt;dbl&gt; 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n$ lead_time         &lt;dbl&gt; 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n$ in_transit_qty    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ forecast_3_month  &lt;dbl&gt; 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n$ forecast_6_month  &lt;dbl&gt; 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n$ forecast_9_month  &lt;dbl&gt; 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n$ sales_1_month     &lt;dbl&gt; 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n$ sales_3_month     &lt;dbl&gt; 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n$ sales_6_month     &lt;dbl&gt; 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n$ sales_9_month     &lt;dbl&gt; 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n$ min_bank          &lt;dbl&gt; 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n$ potential_issue   &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ pieces_past_due   &lt;dbl&gt; 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ perf_6_month_avg  &lt;dbl&gt; 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n$ perf_12_month_avg &lt;dbl&gt; 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n$ local_bo_qty      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n$ deck_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ oe_constraint     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ ppap_risk         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n$ stop_auto_buy     &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ rev_stop          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ went_on_backorder &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n\n\n\nrecipe_obj &lt;- recipe(went_on_backorder ~ ., data = product_backorders_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, fn = as.factor) %&gt;%\n  prep()\n\n\nset.seed(1234)\n\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.85)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl &lt;- testing(split_obj)\n\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\n\nh2o.init()\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\RtmpotJ9VK\\file17fc6018f3/h2o_Lenovo_started_from_r.out\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\RtmpotJ9VK\\file17fc1fcc3462/h2o_Lenovo_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         4 seconds 81 milliseconds \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    6 months and 5 days \n    H2O cluster name:           H2O_started_from_R_Lenovo_pgz902 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.74 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.0 (2023-04-21 ucrt) \n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5 \n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n11:22:50.406: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n11:22:50.432: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\ntypeof(automl_models_h2o)\n\n[1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n[1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n[5] \"modeling_steps\" \"training_info\" \n\nautoml_models_h2o@leaderboard\n\n                                                 model_id       auc   logloss\n1 StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 0.9561234 0.1719370\n2    StackedEnsemble_AllModels_3_AutoML_1_20240626_112250 0.9560200 0.1708690\n3    StackedEnsemble_AllModels_2_AutoML_1_20240626_112250 0.9558390 0.1710644\n4    StackedEnsemble_AllModels_1_AutoML_1_20240626_112250 0.9538012 0.1738048\n5 StackedEnsemble_BestOfFamily_2_AutoML_1_20240626_112250 0.9530344 0.1764172\n6                          GBM_4_AutoML_1_20240626_112250 0.9520364 0.1791385\n      aucpr mean_per_class_error      rmse        mse\n1 0.7587667            0.1479800 0.2289585 0.05242201\n2 0.7665524            0.1492160 0.2278504 0.05191579\n3 0.7655005            0.1487976 0.2280735 0.05201754\n4 0.7587346            0.1506517 0.2294995 0.05267003\n5 0.7464036            0.1331207 0.2313708 0.05353243\n6 0.7409863            0.1284338 0.2332589 0.05440969\n\n[23 rows x 7 columns] \n\nautoml_models_h2o@leader\n\nModel Details:\n==============\n\nH2OBinomialModel: stackedensemble\nModel ID:  StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 \nModel Summary for Stacked Ensemble: \n                                         key            value\n1                          Stacking strategy cross_validation\n2       Number of base models (used / total)              2/5\n3           # GBM base models (used / total)              1/1\n4           # DRF base models (used / total)              1/2\n5  # DeepLearning base models (used / total)              0/1\n6           # GLM base models (used / total)              0/1\n7                      Metalearner algorithm              GLM\n8         Metalearner fold assignment scheme           Random\n9                         Metalearner nfolds                5\n10                   Metalearner fold_column               NA\n11        Custom metalearner hyperparameters             None\n\n\nH2OBinomialMetrics: stackedensemble\n** Reported on training data. **\n\nMSE:  0.01932387\nRMSE:  0.1390103\nLogLoss:  0.07927591\nMean Per-Class Error:  0.05012764\nAUC:  0.9950306\nAUCPR:  0.9716874\nGini:  0.9900613\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         No  Yes    Error        Rate\nNo     8700  103 0.011701   =103/8803\nYes     106 1091 0.088555   =106/1197\nTotals 8806 1194 0.020900  =209/10000\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold       value idx\n1                       max f1  0.424060    0.912589 173\n2                       max f2  0.319633    0.922211 202\n3                 max f0point5  0.572323    0.936852 134\n4                 max accuracy  0.441319    0.979200 168\n5                max precision  0.993213    1.000000   0\n6                   max recall  0.040059    1.000000 324\n7              max specificity  0.993213    1.000000   0\n8             max absolute_mcc  0.424060    0.900721 173\n9   max min_per_class_accuracy  0.241678    0.962406 227\n10 max mean_per_class_accuracy  0.231269    0.963242 231\n11                     max tns  0.993213 8803.000000   0\n12                     max fns  0.993213 1194.000000   0\n13                     max fps  0.000338 8803.000000 399\n14                     max tps  0.040059 1197.000000 324\n15                     max tnr  0.993213    1.000000   0\n16                     max fnr  0.993213    0.997494   0\n17                     max fpr  0.000338    1.000000 399\n18                     max tpr  0.040059    1.000000 324\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: stackedensemble\n** Reported on validation data. **\n\nMSE:  0.04593089\nRMSE:  0.2143149\nLogLoss:  0.1549916\nMean Per-Class Error:  0.152176\nAUC:  0.9575806\nAUCPR:  0.7697308\nGini:  0.9151613\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         No Yes    Error       Rate\nNo     2040  89 0.041804   =89/2129\nYes      68 191 0.262548    =68/259\nTotals 2108 280 0.065745  =157/2388\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold       value idx\n1                       max f1  0.396232    0.708720 160\n2                       max f2  0.166073    0.776732 240\n3                 max f0point5  0.686776    0.744337  91\n4                 max accuracy  0.522239    0.939698 132\n5                max precision  0.987921    1.000000   0\n6                   max recall  0.013005    1.000000 361\n7              max specificity  0.987921    1.000000   0\n8             max absolute_mcc  0.522239    0.675954 132\n9   max min_per_class_accuracy  0.166073    0.891892 240\n10 max mean_per_class_accuracy  0.132816    0.897120 254\n11                     max tns  0.987921 2129.000000   0\n12                     max fns  0.987921  258.000000   0\n13                     max fps  0.000173 2129.000000 399\n14                     max tps  0.013005  259.000000 361\n15                     max tnr  0.987921    1.000000   0\n16                     max fnr  0.987921    0.996139   0\n17                     max fpr  0.000173    1.000000 399\n18                     max tpr  0.013005    1.000000 361\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: stackedensemble\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.05090027\nRMSE:  0.2256109\nLogLoss:  0.1725378\nMean Per-Class Error:  0.1609015\nAUC:  0.9499098\nAUCPR:  0.7460786\nGini:  0.8998197\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n          No  Yes    Error         Rate\nNo     11600  553 0.045503   =553/12153\nYes      457 1197 0.276300    =457/1654\nTotals 12057 1750 0.073151  =1010/13807\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold        value idx\n1                       max f1  0.350194     0.703290 201\n2                       max f2  0.164018     0.777717 265\n3                 max f0point5  0.562723     0.738517 137\n4                 max accuracy  0.540548     0.932498 144\n5                max precision  0.988893     1.000000   0\n6                   max recall  0.000456     1.000000 399\n7              max specificity  0.988893     1.000000   0\n8             max absolute_mcc  0.411648     0.662743 183\n9   max min_per_class_accuracy  0.114396     0.883979 286\n10 max mean_per_class_accuracy  0.105893     0.886304 290\n11                     max tns  0.988893 12153.000000   0\n12                     max fns  0.988893  1650.000000   0\n13                     max fps  0.000456 12153.000000 399\n14                     max tps  0.000456  1654.000000 399\n15                     max tnr  0.988893     1.000000   0\n16                     max fnr  0.988893     0.997582   0\n17                     max fpr  0.000456     1.000000 399\n18                     max tpr  0.000456     1.000000 399\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nCross-Validation Metrics Summary: \n                mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\naccuracy    0.928743  0.007437   0.936990   0.918571   0.934186   0.929623\nauc         0.950190  0.002653   0.954707   0.950147   0.948655   0.949449\nerr         0.071257  0.007437   0.063010   0.081429   0.065814   0.070377\nerr_count 197.000000 23.323807 170.000000 228.000000 180.000000 196.000000\nf0point5    0.700801  0.027873   0.722807   0.659824   0.730230   0.692137\n          cv_5_valid\naccuracy    0.924346\nauc         0.947989\nerr         0.075654\nerr_count 211.000000\nf0point5    0.699009\n\n---\n                        mean        sd cv_1_valid cv_2_valid cv_3_valid\nprecision           0.693693  0.043396   0.733096   0.633803   0.738710\nr2                  0.517460  0.006031   0.518965   0.506979   0.518830\nrecall              0.737920  0.045015   0.684385   0.789474   0.698171\nresidual_deviance 952.444000 54.130043 870.332340 988.683350 946.922700\nrmse                0.225448  0.005405   0.218355   0.229921   0.225355\nspecificity         0.954526  0.013208   0.968711   0.936534   0.966348\n                  cv_4_valid  cv_5_valid\nprecision           0.675676    0.687180\nr2                  0.522391    0.520138\nrecall              0.766871    0.750700\nresidual_deviance 944.129760 1012.151800\nrmse                0.222177    0.231433\nspecificity         0.951200    0.949836\n\ntypeof(automl_models_h2o@leader)\n\n[1] \"S4\"\n\n\n\npredictions &lt;- h2o.predict(automl_models_h2o@leader, newdata = as.h2o(test_tbl))\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl &lt;- \n  predictions %&gt;% \n    as_tibble()\n#h2o.saveModel(automl_models_h2o@leader, path = \"./04_perf_meas_files/\")\n\n\npredictions_tbl %&gt;%\n  glimpse()\n\nRows: 2,858\nColumns: 3\n$ predict &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Yes,…\n$ No      &lt;dbl&gt; 0.35334247, 0.36027327, 0.16122040, 0.18614910, 0.15906951, 0.…\n$ Yes     &lt;dbl&gt; 0.64665753, 0.63972673, 0.83877960, 0.81385090, 0.84093049, 0.…\n\n\n\nautoml_models_h2o@leaderboard %&gt;% \n              as_tibble() %&gt;% \n              select(-c(mean_per_class_error, rmse, mse))\n\n# A tibble: 23 × 4\n   model_id                                                  auc logloss aucpr\n   &lt;chr&gt;                                                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 StackedEnsemble_BestOfFamily_3_AutoML_1_20240626_112250 0.956   0.172 0.759\n 2 StackedEnsemble_AllModels_3_AutoML_1_20240626_112250    0.956   0.171 0.767\n 3 StackedEnsemble_AllModels_2_AutoML_1_20240626_112250    0.956   0.171 0.766\n 4 StackedEnsemble_AllModels_1_AutoML_1_20240626_112250    0.954   0.174 0.759\n 5 StackedEnsemble_BestOfFamily_2_AutoML_1_20240626_112250 0.953   0.176 0.746\n 6 GBM_4_AutoML_1_20240626_112250                          0.952   0.179 0.741\n 7 GBM_grid_1_AutoML_1_20240626_112250_model_1             0.951   0.178 0.744\n 8 GBM_grid_1_AutoML_1_20240626_112250_model_2             0.951   0.179 0.755\n 9 GBM_1_AutoML_1_20240626_112250                          0.950   0.179 0.753\n10 StackedEnsemble_BestOfFamily_1_AutoML_1_20240626_112250 0.950   0.179 0.753\n# ℹ 13 more rows\n\n\n\nplot_h2o_leaderboard &lt;- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n\n    # Setup inputs\n    # adjust input so that all formats are working\n    order_by &lt;- tolower(order_by[[1]])\n\n    leaderboard_tbl &lt;- h2o_leaderboard %&gt;%\n        as_tibble() %&gt;%\n        select(-c(aucpr, mean_per_class_error, rmse, mse)) %&gt;% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %&gt;%\n        rownames_to_column(var = \"rowname\") %&gt;%\n        mutate(model_id = paste0(rowname, \". \", model_id) %&gt;% as.factor())\n\n    # Transformation\n    if (order_by == \"auc\") {\n\n        data_transformed_tbl &lt;- leaderboard_tbl %&gt;%\n            slice(1:n_max) %&gt;%\n            mutate(\n                model_id   = as_factor(model_id) %&gt;% reorder(auc),\n                model_type = as.factor(model_type)\n            ) %&gt;%\n                pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else if (order_by == \"logloss\") {\n\n        data_transformed_tbl &lt;- leaderboard_tbl %&gt;%\n            slice(1:n_max) %&gt;%\n            mutate(\n                model_id   = as_factor(model_id) %&gt;% reorder(logloss) %&gt;% fct_rev(),\n                model_type = as.factor(model_type)\n            ) %&gt;%\n            pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else {\n        # If nothing is supplied\n        stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n    }\n\n    # Visualization\n    g &lt;- data_transformed_tbl %&gt;%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = size) +\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n             y = \"Model Postion, Model ID\", x = \"\")\n\n    if (include_lbl) g &lt;- g + geom_label(aes(label = round(value, 2), \n                                             hjust = \"inward\"))\n\n    return(g)\n\n}\n\n\nautoml_models_h2o@leaderboard %&gt;% plot_h2o_leaderboard()\n\n\n\n\n\n\n\n\nI was unable to build the page because the program have some problem with an h2o function. So I saved the results and loaded them. The used code is commented out for a better understanding.\n\n#h2o.init()\n#deeplearning_h2o &lt;- \n#h2o.loadModel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/DeepLearning_1_AutoML_3_20220614_234925\")\n#deeplearning_h2o@allparameters\n\n#Deeplearning_grid_01 &lt;- h2o.grid()\n \n#     # See help page for available algos\n    #algorithm = \"deeplearning\"\n#     \n#     # I just use the same as the object\n    #grid_id = \"Deaplearning_grid_01\"\n#     \n#     # The following is for ?h2o.deeplearning()\n#     # predictor and response variables\n     #x = x,\n     #y = y,\n#     \n#     # training and validation frame and crossfold validation\n#     training_frame   = train_h2o,\n#     validation_frame = valid_h2o,\n#     nfolds = 5,\n#     \n#     # Hyperparamters: Use deeplearning_h2o@allparameters to see all\n#     hyper_params = list(\n#         # Use some combinations (the first one was the original)\n#         hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n#         epochs = c(10, 50, 100)\n#     )\n# )\n\n\n# &lt;- h2o.getModel(\"Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 %&gt;%h2o.saveModel(path = \"04_Modeling/Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 &lt;- h2o.loadModel(\"04_Modeling/Deaplearning_grid_01_model_3/Deaplearning_grid_01_model_3\")\n# performance_h2o &lt;- h2o.performance(Deeplearning_grid_01_model_3, newdata = as.h2o(test_tbl))\n# \n# performance_tbl &lt;- performance_h2o %&gt;%\n#     h2o.metric() %&gt;%\n#     as.tibble()\n# \n# theme_new &lt;- theme(\n#       legend.position  = \"bottom\",\n#       panel.background = element_rect(fill   = \"transparent\"),\n#       panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n#       panel.grid.major = element_line(color = \"grey\", size = 0.333)\n#       ) \n #saveRDS(performance_tbl, file = \"performance_tbl.rds\")\n\n#performance_tbl &lt;- readRDS(\"performance_tbl.rds\")\n\n\n#performance_tbl %&gt;%\n    #filter(f1 == max(f1))\n\n#performance_tbl %&gt;%\n    #ggplot(aes(x = threshold)) +\n    #geom_line(aes(y = precision), color = \"blue\", size = 1) +\n    #geom_line(aes(y = recall), color = \"red\", size = 1) +\n    \n    # Insert line where precision and recall are harmonically optimized\n    #geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n    #labs(title = \"Precision vs Recall\", y = \"value\") +\n    #theme_new\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-15-1.png\")\n\n\n\n\n\n\n\n\n\n#p1 &lt;- performance_tbl %&gt;%\n  #ggplot(aes(fpr, tpr)) +\n    #geom_line(size = 1) +\n    \n    # just for demonstration purposes\n    #geom_abline(color = \"red\", linetype = \"dotted\") +\n    \n    #theme_new +\n    #theme(\n      #legend.direction = \"vertical\",\n      #) +\n    #labs(\n        #title = \"ROC Plot\"\n        #subtitle = \"Performance of 3 Top Performing Models\"\n   # )\n#p1\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-16-1.png\")\n\n\n\n\n\n\n\n\n\n#p2 &lt;- performance_tbl %&gt;%\n  #ggplot(aes(recall, precision)) +\n    #geom_line(size = 1) +\n    #theme_new + \n    #theme(\n      #legend.direction = \"vertical\",\n      #) +\n    #labs(\n        #title = \"Precision vs Recall Plot\"\n        #subtitle = \"Performance of 3 Top Performing Models\"\n    #)\n#p2\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-17-1.png\")\n\n\n\n\n\n\n\n\n\nranked_predictions_tbl &lt;- predictions_tbl %&gt;%\n    bind_cols(test_tbl) %&gt;%\n    select(predict:Yes, went_on_backorder) %&gt;%\n    # Sorting from highest to lowest class probability\n    arrange(desc(Yes))\n\ncalculated_gain_lift_tbl &lt;- ranked_predictions_tbl %&gt;%\n    mutate(ntile = ntile(Yes, n = 10)) %&gt;%\n    group_by(ntile) %&gt;%\n    summarise(\n        cases = n(),\n        responses = sum(went_on_backorder == \"Yes\")\n    ) %&gt;%\n    arrange(desc(ntile)) %&gt;%\n    \n    # Add group numbers (opposite of ntile)\n    mutate(group = row_number()) %&gt;%\n    select(group, cases, responses) %&gt;%\n    \n    # Calculations\n    mutate(\n        cumulative_responses = cumsum(responses),\n        pct_responses        = responses / sum(responses),\n        gain                 = cumsum(pct_responses),\n        cumulative_pct_cases = cumsum(cases) / sum(cases),\n        lift                 = gain / cumulative_pct_cases,\n        gain_baseline        = cumulative_pct_cases,\n        lift_baseline        = gain_baseline / cumulative_pct_cases\n    )\n\n\n#gain_lift_tbl &lt;- performance_h2o %&gt;%\n    #h2o.gainsLift() %&gt;%\n    #as.tibble()\n\n#gain_transformed_tbl &lt;- gain_lift_tbl %&gt;% \n    #select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %&gt;%\n    #select(-contains(\"lift\")) %&gt;%\n    #mutate(baseline = cumulative_data_fraction) %&gt;%\n    #rename(gain     = cumulative_capture_rate) %&gt;%\n    # prepare the data for the plotting (for the color and group aesthetics)\n    #pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\n#p3 &lt;- gain_transformed_tbl %&gt;%\n    #ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n    #geom_line(size = 1.5) +\n    #labs(\n        #title = \"Gain Chart\",\n        #x = \"Cumulative Data Fraction\",\n        #y = \"Gain\"\n    #) +\n    #theme_new\n#p3\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-19-1.png\")\n\n\n\n\n\n\n\n\n\n#lift_transformed_tbl &lt;- gain_lift_tbl %&gt;% \n#    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %&gt;%\n#    select(-contains(\"capture\")) %&gt;%\n#    mutate(baseline = 1) %&gt;%\n#    rename(lift = cumulative_lift) %&gt;%\n#    pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n#\n#p4 &lt;- lift_transformed_tbl %&gt;%\n#    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n#    geom_line(size = 1.5) +\n#    labs(\n#        title = \"Lift Chart\",\n#        x = \"Cumulative Data Fraction\",\n#        y = \"Lift\"\n#    ) +\n#    theme_new\n#p4\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-20-1.png\")\n\n\n\n\n\n\n\n\n\nlibrary(cowplot)\nlibrary(glue)\n\n\n# Combine using cowplot\n   # \n#    # cowplot::get_legend extracts a legend from a ggplot object\n  #  p_legend &lt;- get_legend(p1)\n#    # Remove legend from p1\n #   p1 &lt;- p1 + theme(legend.position = \"none\")\n    \n    # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n#    p &lt;- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n #   p\nknitr::include_graphics(\"E:/Fahad/bdml-injolifi/04_perf_meas_files/figure-html/unnamed-chunk-21-1.png\")"
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html",
    "href": "Chapter_1_Challenge_fahad.html",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "",
    "text": "Your organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Salesforce CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nYou will be using stock prices in this analysis. You come up with a method to classify companies based on how their stocks trade using their daily stock returns (percentage movement from one day to the next). This analysis will help your organization determine which companies are related to each other (competitors and have similar attributes).\nYou can analyze the stock prices using what you’ve learned in the unsupervised learning tools including K-Means and UMAP. You will use a combination of kmeans() to find groups and umap() to visualize similarity of daily stock returns."
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "Chapter_1_Challenge_fahad.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat you first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\]\nFirst, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %&gt;% glimpse()\n\nRows: 1,225,765\nColumns: 8\n$ symbol   &lt;chr&gt; \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n$ date     &lt;date&gt; 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n$ open     &lt;dbl&gt; 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n$ high     &lt;dbl&gt; 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n$ low      &lt;dbl&gt; 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n$ close    &lt;dbl&gt; 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n$ volume   &lt;dbl&gt; 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n$ adjusted &lt;dbl&gt; 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\nYour first task is to convert to a tibble named sp_500_daily_returns_tbl by performing the following operations:\n\nSelect the symbol, date and adjusted columns\nFilter to dates beginning in the year 2018 and beyond.\nCompute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame.\nRemove a NA values from the lagging operation\nCompute the difference between adjusted and the lag\nCompute the percentage difference by dividing the difference by that lag. Name this column pct_return.\nReturn only the symbol, date, and pct_return columns\nSave as a variable named sp_500_daily_returns_tbl\n\n\n# Apply your data transformation skills!\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;%\n  select(symbol, date, adjusted) %&gt;%\n  filter(date &gt;= '2018-01-01' & date &lt;= '2018-12-31') %&gt;%\n  group_by(symbol) %&gt;%\n  mutate(lag = lag(adjusted, n = 1, default = NA)) %&gt;%\n  ungroup() %&gt;%\n  drop_na() %&gt;%\n  mutate(difference = adjusted - lag) %&gt;%\n  mutate(pct_return = difference/lag) %&gt;%\n  select(symbol, date, pct_return)\n\n# Output: sp_500_daily_returns_tbl"
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-2---convert-to-user-item-format",
    "href": "Chapter_1_Challenge_fahad.html#step-2---convert-to-user-item-format",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 2 - Convert to User-Item Format",
    "text": "Step 2 - Convert to User-Item Format\nThe next step is to convert to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nsp_500_daily_returns_tbl &lt;- read_rds(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n# A tibble: 141,340 × 3\n   symbol date       pct_return\n   &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n 1 MSFT   2018-01-03   0.00465 \n 2 MSFT   2018-01-04   0.00880 \n 3 MSFT   2018-01-05   0.0124  \n 4 MSFT   2018-01-08   0.00102 \n 5 MSFT   2018-01-09  -0.000680\n 6 MSFT   2018-01-10  -0.00453 \n 7 MSFT   2018-01-11   0.00296 \n 8 MSFT   2018-01-12   0.0173  \n 9 MSFT   2018-01-16  -0.0140  \n10 MSFT   2018-01-17   0.0203  \n# ℹ 141,330 more rows\n\n\nNow that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\n\nSpread the date column to get the values as percentage returns. Make sure to fill an NA values with zeros.\nSave the result as stock_date_matrix_tbl\n\n\n# Convert to User-Item Format\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;%\n  arrange(symbol) %&gt;%\n  pivot_wider(names_from =date, values_from = pct_return, values_fill = 0)\n\nstock_date_matrix_tbl \n\n# A tibble: 502 × 283\n   symbol `2018-01-03` `2018-01-04` `2018-01-05` `2018-01-08` `2018-01-09`\n   &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 A          0.0254       -0.00750     0.0160        0.00215     0.0246  \n 2 AAL       -0.0123        0.00630    -0.000380     -0.00988    -0.000959\n 3 AAP        0.00905       0.0369      0.0106       -0.00704    -0.00808 \n 4 AAPL      -0.000174      0.00465     0.0114       -0.00371    -0.000115\n 5 ABBV       0.0156       -0.00570     0.0174       -0.0160      0.00754 \n 6 ABC        0.00372      -0.00222     0.0121        0.0166      0.00640 \n 7 ABMD       0.0173        0.0175      0.0154        0.0271      0.00943 \n 8 ABT        0.00221      -0.00170     0.00289      -0.00288     0.00170 \n 9 ACN        0.00462       0.0118      0.00825       0.00799     0.00333 \n10 ADBE       0.0188        0.0120      0.0116       -0.00162     0.00897 \n# ℹ 492 more rows\n# ℹ 277 more variables: `2018-01-10` &lt;dbl&gt;, `2018-01-11` &lt;dbl&gt;,\n#   `2018-01-12` &lt;dbl&gt;, `2018-01-16` &lt;dbl&gt;, `2018-01-17` &lt;dbl&gt;,\n#   `2018-01-18` &lt;dbl&gt;, `2018-01-19` &lt;dbl&gt;, `2018-01-22` &lt;dbl&gt;,\n#   `2018-01-23` &lt;dbl&gt;, `2018-01-24` &lt;dbl&gt;, `2018-01-25` &lt;dbl&gt;,\n#   `2018-01-26` &lt;dbl&gt;, `2018-01-29` &lt;dbl&gt;, `2018-01-30` &lt;dbl&gt;,\n#   `2018-01-31` &lt;dbl&gt;, `2018-02-01` &lt;dbl&gt;, `2018-02-02` &lt;dbl&gt;, …\n\n# Output: stock_date_matrix_tbl"
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-3---perform-k-means-clustering",
    "href": "Chapter_1_Challenge_fahad.html#step-3---perform-k-means-clustering",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 3 - Perform K-Means Clustering",
    "text": "Step 3 - Perform K-Means Clustering\nNext, we’ll perform K-Means clustering.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nstock_date_matrix_tbl &lt;- read_rds(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/stock_date_matrix_tbl.rds\")\n\nBeginning with the stock_date_matrix_tbl, perform the following operations:\n\nDrop the non-numeric column, symbol\nPerform kmeans() with centers = 4 and nstart = 20\nSave the result as kmeans_obj\n\n\n# Create kmeans_obj for 4 centers\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;%\n  select(-symbol) %&gt;%\n  kmeans(centers = 4, nstart = 20)\n\nUse glance() to get the tot.withinss.\n\n# Apply glance() to get the tot.withinss\nbroom::glance(kmeans_obj)\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1  33.6         29.2      4.40     3"
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-4---find-the-optimal-value-of-k",
    "href": "Chapter_1_Challenge_fahad.html#step-4---find-the-optimal-value-of-k",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 4 - Find the optimal value of K",
    "text": "Step 4 - Find the optimal value of K\nNow that we are familiar with the process for calculating kmeans(), let’s use purrr to iterate over many values of “k” using the centers argument.\nWe’ll use this custom function called kmeans_mapper():\n\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)\n}\n\nApply the kmeans_mapper() and glance() functions iteratively using purrr.\n\nCreate a tibble containing column called centers that go from 1 to 30\nAdd a column named k_means with the kmeans_mapper() output. Use mutate() to add the column and map() to map centers to the kmeans_mapper() function.\nAdd a column named glance with the glance() output. Use mutate() and map() again to iterate over the column of k_means.\nSave the output as k_means_mapped_tbl\n\n\n# Use purrr to map\nkmeans_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\n\n# Output: k_means_mapped_tbl \n\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nBegin with the k_means_mapped_tbl\nUnnest the glance column\nPlot the centers column (x-axis) versus the tot.withinss column (y-axis) using geom_point() and geom_line()\nAdd a title “Scree Plot” and feel free to style it with your favorite theme\n\n\n# Visualize Scree Plot\nkmeans_mapped_tbl %&gt;%\n    unnest(glance) %&gt;%\n    select(centers, tot.withinss) %&gt;%\n    \n    # Visualization\n    ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"#2DC6D6\", size = 4) +\n    geom_line(color = \"#2DC6D6\", size = 1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-5---apply-umap",
    "href": "Chapter_1_Challenge_fahad.html#step-5---apply-umap",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 5 - Apply UMAP",
    "text": "Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\nk_means_mapped_tbl\n\n# A tibble: 30 × 3\n   centers k_means  glance          \n     &lt;int&gt; &lt;list&gt;   &lt;list&gt;          \n 1       1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 2       2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 3       3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 4       4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 5       5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 6       6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 7       7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 8       8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n 9       9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n10      10 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt;\n# ℹ 20 more rows\n\n\nFirst, let’s apply the umap() function to the stock_date_matrix_tbl, which contains our user-item matrix in tibble format.\n\nStart with stock_date_matrix_tbl\nDe-select the symbol column\nUse the umap() function storing the output as umap_results\n\n\n# Apply UMAP\numap_results &lt;- stock_date_matrix_tbl %&gt;%\n  select(-symbol) %&gt;%\n  umap()\n# Store results as: umap_results \n\nNext, we want to combine the layout from the umap_results with the symbol column from the stock_date_matrix_tbl.\n\nStart with umap_results$layout\nConvert from a matrix data type to a tibble with as_tibble()\nBind the columns of the umap tibble with the symbol column from the stock_date_matrix_tbl.\nSave the results as umap_results_tbl.\n\n\n# Convert umap results to tibble with symbols\numap_results_tbl &lt;- umap_results$layout %&gt;%\n  as_tibble() %&gt;%\n  cbind(stock_date_matrix_tbl$symbol)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\numap_results_tbl\n\n              V1           V2 stock_date_matrix_tbl$symbol\n1   -1.073107311  1.926631527                            A\n2    1.324669170  1.598854242                          AAL\n3    0.328246595 -0.347761344                          AAP\n4   -2.196437920  2.551530958                         AAPL\n5   -1.608496148 -0.088849432                         ABBV\n6   -1.052260670 -0.600668455                          ABC\n7   -2.545593145  2.008546553                         ABMD\n8   -1.546202420  1.188429326                          ABT\n9   -1.336934194  1.409834424                          ACN\n10  -2.690776164  2.429016106                         ADBE\n11  -1.588333421  3.419118151                          ADI\n12   0.220845868 -0.900078375                          ADM\n13  -1.644697631  1.461241290                          ADP\n14  -0.579858932  1.162159134                          ADS\n15  -2.742462654  2.307582021                         ADSK\n16   3.048660709 -3.355681993                          AEE\n17   3.175081295 -3.376210291                          AEP\n18   3.127614417 -3.434693573                          AES\n19  -0.799818097 -1.236312854                          AFL\n20  -1.138215256 -1.491930444                          AGN\n21  -1.095736154 -1.680394233                          AIG\n22   2.564845013 -6.125178256                          AIV\n23  -0.997085105 -1.259257031                          AIZ\n24  -1.043716415 -0.827865173                          AJG\n25  -1.180781000  1.493195519                         AKAM\n26   0.751536076  2.368825395                          ALB\n27  -2.744050900  2.024674725                         ALGN\n28   1.318478021  1.557991165                          ALK\n29  -0.879498002 -1.064614770                          ALL\n30   0.181501426  2.174856875                         ALLE\n31  -2.211097430  1.806104579                         ALXN\n32  -1.596274779  3.506320098                         AMAT\n33  -2.818000468  2.171736420                          AMD\n34   0.104146905  2.406937258                          AME\n35  -2.219915431 -1.987548551                          AMG\n36  -1.729030179  0.007181482                         AMGN\n37  -1.594917704 -2.076064189                          AMP\n38   2.601562479 -5.798341324                          AMT\n39  -2.757613125  2.653749697                         AMZN\n40  -2.576338365  2.301016772                         ANET\n41  -2.471361193  2.353175150                         ANSS\n42  -0.818521940  0.126013315                         ANTM\n43  -0.999366611 -0.770856102                          AON\n44   0.528701991  1.759558573                          AOS\n45   4.297342968  1.435413671                          APA\n46   4.407618279  1.362170757                          APC\n47   0.297267764  1.676982737                          APD\n48  -0.805861645  2.104534815                          APH\n49  -0.065839399  2.219008930                         APTV\n50   2.566020882 -6.329702250                          ARE\n51   0.114169911  2.621354369                         ARNC\n52   2.881647141 -3.316265125                          ATO\n53  -2.478956151  2.816032488                         ATVI\n54   2.679537582 -6.090461293                          AVB\n55  -1.130998554  1.896661394                         AVGO\n56   0.009213295  1.993675365                          AVY\n57   3.083325368 -3.322428693                          AWK\n58  -2.177230766 -1.669316032                          AXP\n59   0.404001217 -0.273632043                          AZO\n60  -0.679886090  2.157317544                           BA\n61  -2.199572162 -2.322899848                          BAC\n62  -1.204759877  0.854247921                          BAX\n63  -2.323922782 -2.752949710                          BBT\n64   0.716056850  0.405016397                          BBY\n65  -1.115672422  0.914615348                          BDX\n66  -2.076163291 -1.922105765                          BEN\n67  -1.597997753 -2.214383939                          BHF\n68   4.046698512  1.465305183                         BHGE\n69  -1.780328890  0.030437595                         BIIB\n70  -2.128492510 -2.305166890                           BK\n71  -1.518966607  1.846251148                         BKNG\n72  -2.266203497 -1.934702955                          BLK\n73  -0.185786507  0.525400475                          BLL\n74  -1.454209338  0.190929968                          BMY\n75  -1.056721398  1.204144395                           BR\n76  -1.552330510  1.067059979                          BSX\n77   0.062079126  2.292495302                          BWA\n78   2.435005435 -6.305471667                          BXP\n79  -1.876296013 -2.179754626                            C\n80   1.567178724 -2.046424659                          CAG\n81  -1.034054401 -0.571121412                          CAH\n82   0.374432769  2.881512588                          CAT\n83  -0.865974502 -1.253528439                           CB\n84  -1.498508825 -1.018423015                         CBOE\n85   0.051015893  1.867140567                         CBRE\n86  -0.323626793 -1.118950637                          CBS\n87   2.579263033 -5.652452365                          CCI\n88  -2.140321241  2.376800356                         CDNS\n89   0.626831051  2.001220943                           CE\n90  -1.750788667 -0.064915583                         CELG\n91  -1.287391637  0.294254149                         CERN\n92   0.798445832  1.985358329                           CF\n93  -2.355854581 -2.821703227                          CFG\n94   2.006462517 -2.875845659                          CHD\n95  -0.385487999  1.223598770                         CHRW\n96  -0.491849373 -1.604626109                         CHTR\n97  -0.879378575  0.058030773                           CI\n98  -0.788180222 -1.244383713                         CINF\n99   1.653387688 -2.621952791                           CL\n100  1.969003426 -2.906876149                          CLX\n101 -2.441254596 -2.713292668                          CMA\n102 -0.405237656 -1.441224081                        CMCSA\n103 -1.471607053 -0.939706931                          CME\n104 -0.032786359  0.088274986                          CMG\n105  0.332275619  2.801762077                          CMI\n106  3.143084549 -3.333866581                          CMS\n107 -0.978363873  0.258505128                          CNC\n108  2.967417474 -3.478925294                          CNP\n109 -2.154911194 -1.801824982                          COF\n110  3.657879912  1.431156487                          COG\n111 -1.393096518  1.152757019                          COO\n112  4.285119396  1.363475679                          COP\n113  0.575761769 -0.292110485                         COST\n114 -0.790508482  1.122692724                         COTY\n115  1.978000360 -2.452074114                          CPB\n116  0.032173842  1.324756502                         CPRI\n117 -1.247611621  0.930548776                         CPRT\n118 -2.755893797  2.354886395                          CRM\n119 -1.731727029  2.470961008                         CSCO\n120 -0.469539444  2.124838060                          CSX\n121 -0.729800125  1.155376419                         CTAS\n122  0.426909742 -1.633561511                          CTL\n123 -1.526479415  1.294185921                         CTSH\n124 -0.607549358  0.581287110                         CTXS\n125 -0.998888828 -0.537422844                          CVS\n126  4.008742808  1.495180864                          CVX\n127  4.441501626  1.322013914                          CXO\n128  3.100345973 -3.286165062                            D\n129  1.304965914  1.512473832                          DAL\n130  0.367388326  2.938535254                           DE\n131 -2.191119877 -1.752138649                          DFS\n132  0.881330438 -0.031265726                           DG\n133 -0.403758730 -0.524043739                          DGX\n134  0.941219391  0.883100176                          DHI\n135 -1.257470086  1.091050194                          DHR\n136 -0.235567127 -1.114630248                          DIS\n137 -0.390124158 -1.400217896                        DISCA\n138 -0.612936311 -1.471196906                        DISCK\n139 -0.447510056 -1.685604086                         DISH\n140  2.633276153 -6.156305718                          DLR\n141  0.609402478 -0.037198980                         DLTR\n142  0.145609441  2.508229803                          DOV\n143  2.511204756 -6.056731920                          DRE\n144 -1.052172266  0.025574389                          DRI\n145  3.116787822 -3.328428825                          DTE\n146  3.154337754 -3.319909115                          DUK\n147 -0.717623571 -0.193046977                          DVA\n148  4.371344968  1.411823992                          DVN\n149  0.835699055  2.183089147                         DWDP\n150 -0.903703845  1.359595258                          DXC\n151 -2.645379649  2.675982041                           EA\n152 -0.269435806  2.819018111                         EBAY\n153 -0.269259206  0.300234297                          ECL\n154  3.090606585 -3.546231378                           ED\n155 -0.309817292 -0.583245872                          EFX\n156  2.968374449 -3.534049618                          EIX\n157 -0.545809156  0.857914862                           EL\n158  0.755629380  2.010562083                          EMN\n159  0.394706253  2.496475648                          EMR\n160  4.475760170  1.331703477                          EOG\n161  2.585472810 -6.187104585                         EQIX\n162  2.573059675 -6.108803257                          EQR\n163  2.996056270 -3.299071348                           ES\n164  2.744582994 -6.047460256                          ESS\n165 -2.424549926 -2.218608855                         ETFC\n166  0.189103876  2.899364017                          ETN\n167  2.845601527 -3.469451919                          ETR\n168  2.887233640 -3.211489284                         EVRG\n169 -2.287924874  1.970940483                           EW\n170  2.916923298 -3.134658920                          EXC\n171 -0.186446314  1.817467761                         EXPD\n172 -1.215289407  1.725382120                         EXPE\n173  2.738969497 -6.089810627                          EXR\n174 -1.460939478 -1.484533495                            F\n175  4.435716563  1.282830175                         FANG\n176 -0.443624097  0.766099493                         FAST\n177 -2.584035139  2.514580985                           FB\n178  0.703100563  1.423866891                         FBHS\n179  0.576016606  2.669345863                          FCX\n180 -0.109506335  1.503677022                          FDX\n181  3.146523609 -3.235193452                           FE\n182 -1.585757847  1.762063602                         FFIV\n183 -1.116293097  0.989670266                          FIS\n184 -1.037065595  0.774102356                         FISV\n185 -2.492306258 -2.833565650                         FITB\n186  0.521786182  0.350342148                           FL\n187 -0.999130292  2.276534749                         FLIR\n188  0.980474318  2.435795351                          FLR\n189  0.430076626  2.472110452                          FLS\n190 -2.014776783  1.949745655                          FLT\n191  0.886727096  2.172674096                          FMC\n192 -0.092137076 -0.959463159                          FOX\n193 -0.052068401 -0.914858522                         FOXA\n194 -2.168044874 -2.821256895                          FRC\n195  2.489210050 -6.283504675                          FRT\n196  4.303861111  1.521995575                          FTI\n197 -2.466036869  2.175056522                         FTNT\n198 -0.160377687  2.221952642                          FTV\n199  1.043468606  3.224591698                           GD\n200 -1.061186996 -1.455986057                           GE\n201 -1.788465156 -0.001072917                         GILD\n202  1.815798139 -2.373829895                          GIS\n203 -0.076465502  2.652338284                          GLW\n204 -1.690942793 -1.661417383                           GM\n205 -2.584056610  2.632849903                         GOOG\n206 -2.613088545  2.543550537                        GOOGL\n207  0.452968157 -0.284055705                          GPC\n208 -2.320472425  2.335098951                          GPN\n209  0.980735151  0.116804275                          GPS\n210 -0.404556455  0.942534971                         GRMN\n211 -2.114945317 -2.330003496                           GS\n212  0.954763254  1.951835585                           GT\n213 -0.396321166  0.724769446                          GWW\n214  4.226634122  1.568703924                          HAL\n215 -0.635024803  0.654613858                          HAS\n216 -2.349599084 -2.780058924                         HBAN\n217 -2.366128082 -2.628974160                          HBI\n218  0.049368284 -0.103418643                          HCA\n219  2.672512553 -6.286157389                          HCP\n220  0.652780492  0.315730072                           HD\n221  4.470331312  1.265294728                          HES\n222  3.312908539  1.405335523                          HFC\n223 -0.946031556 -1.291853241                          HIG\n224  1.008850348  3.174405916                          HII\n225 -1.496214769  1.617083905                          HLT\n226 -1.436362717 -2.137904156                          HOG\n227 -0.882968539  1.015830438                         HOLX\n228 -0.208563042  1.457517224                          HON\n229  4.182771188  1.565518429                           HP\n230 -1.392250331  2.580338660                          HPE\n231 -1.428164034  2.843112988                          HPQ\n232 -0.266262258  0.474560078                          HRB\n233  1.735248340 -2.138003718                          HRL\n234  1.019379601  3.271362393                          HRS\n235 -1.417781117  0.569211125                         HSIC\n236  2.731881014 -6.406045416                          HST\n237  2.012137881 -2.450095099                          HSY\n238 -0.950726437  0.067073145                          HUM\n239 -1.090201740  0.661355809                          IBM\n240 -1.514942187 -0.937867770                          ICE\n241 -2.696876245  2.053708480                         IDXX\n242  0.488995720 -1.063159854                          IFF\n243 -1.990669856  2.237661052                         ILMN\n244 -1.826529342  2.529246839                         INCY\n245 -1.150640687  1.386714054                         INFO\n246 -1.543508486  3.141891275                         INTC\n247 -2.641546618  2.272118484                         INTU\n248  0.459632437  2.330712012                           IP\n249 -0.288384095 -1.107227863                          IPG\n250 -0.880124914  2.878813309                         IPGP\n251 -1.021059273  1.736414486                          IQV\n252 -0.106339387  2.575247585                           IR\n253  2.865610406 -5.934531851                          IRM\n254 -2.638128862  2.124457711                         ISRG\n255 -0.896048734  1.029716992                           IT\n256  0.145148247  2.687815017                          ITW\n257 -2.141281985 -1.968768345                          IVZ\n258 -0.416062494  1.820024020                         JBHT\n259  0.301969029  1.582551646                          JCI\n260 -0.004134780  1.964781088                          JEC\n261 -1.809727712 -1.854417308                          JEF\n262 -1.110692004  1.015695570                         JKHY\n263 -1.255333425 -0.342112788                          JNJ\n264 -0.890199153  1.407720582                         JNPR\n265 -2.155829757 -2.185131754                          JPM\n266  1.081194534  0.091979760                          JWN\n267  1.804755423 -2.346992832                            K\n268 -2.544290052 -2.729723298                          KEY\n269 -1.665722381  1.882744042                         KEYS\n270  1.476370362 -2.281818788                          KHC\n271  2.425962674 -6.220532441                          KIM\n272 -1.580531893  3.530663649                         KLAC\n273  2.103477685 -3.051977131                          KMB\n274  3.739269445  1.430976995                          KMI\n275  0.404687432  0.262457864                          KMX\n276  0.959528929 -1.799380315                           KO\n277  0.676441149 -0.341317959                           KR\n278  1.088254615  0.018682279                          KSS\n279 -0.404540435  2.087847968                          KSU\n280 -0.876240373 -1.400299476                            L\n281  1.019295939 -0.036842107                           LB\n282  0.675294619  1.313308975                          LEG\n283  0.948793448  0.915218902                          LEN\n284 -0.566571929 -0.508055962                           LH\n285  0.068315660  0.858592421                          LIN\n286 -0.110940496  2.541223369                          LKQ\n287  1.071679091  3.263526652                          LLL\n288 -1.534702310 -0.258347035                          LLY\n289  1.174380805  3.368701363                          LMT\n290 -1.569215037 -2.171314972                          LNC\n291  3.067571307 -3.253426713                          LNT\n292  0.451835566  0.497073500                          LOW\n293 -1.607796938  3.570794887                         LRCX\n294  1.206996145  1.458552862                          LUV\n295 -0.013988133 -0.369592933                           LW\n296  0.836165730  2.146899247                          LYB\n297  1.099714833 -0.001062634                            M\n298 -2.390496479  2.477037116                           MA\n299  2.624398879 -6.055644385                          MAA\n300  2.286798731 -6.293917585                          MAC\n301 -1.544942060  1.684784054                          MAR\n302  0.604153730  1.515331876                          MAS\n303 -1.390958609  1.397432896                          MAT\n304 -0.396869193 -0.650015032                          MCD\n305 -1.587802699  3.382665760                         MCHP\n306 -1.016559474 -0.501492356                          MCK\n307 -1.959882268  1.471776709                          MCO\n308  1.199384604 -2.026387783                         MDLZ\n309 -1.293784893  0.865184272                          MDT\n310 -1.470989287 -2.248547039                          MET\n311 -0.233112516  2.506911633                          MGM\n312  0.769014446  1.334604577                          MHK\n313  1.673397447 -2.195579872                          MKC\n314  0.392899020  1.100382041                          MLM\n315 -1.052218371 -0.860596868                          MMC\n316  0.378116860  2.636253890                          MMM\n317 -1.030073882  0.196036871                         MNST\n318  1.143580519 -2.163431027                           MO\n319  0.836566242  1.980317329                          MOS\n320  3.267195765  1.440472524                          MPC\n321 -1.335632783 -0.265765319                          MRK\n322  4.411916937  1.309066860                          MRO\n323 -2.262720383 -2.244928320                           MS\n324 -2.093165885  1.643644582                         MSCI\n325 -2.484920893  2.464526247                         MSFT\n326 -1.189042594  1.312322782                          MSI\n327 -2.349375059 -2.817381097                          MTB\n328 -1.099998421  1.754248756                          MTD\n329 -1.548044021  3.572809297                           MU\n330 -1.688271932  3.463588408                         MXIM\n331 -1.871290745 -1.722050326                          MYL\n332  4.507200786  1.369202303                          NBL\n333 -1.333396989  1.940171190                         NCLH\n334 -1.543955986 -0.979407808                         NDAQ\n335  3.067487651 -3.423527662                          NEE\n336  0.095358444 -0.466672456                          NEM\n337 -2.691035718  2.742234778                         NFLX\n338  3.140807046 -3.271392154                           NI\n339 -0.228523494  1.339834422                          NKE\n340 -1.581576064  0.773102554                         NKTR\n341 -1.828970515 -1.450666260                         NLSN\n342  1.130519573  3.305137811                          NOC\n343  4.264490042  1.551822056                          NOV\n344  2.818464992 -2.840845242                          NRG\n345 -0.419942332  2.223211282                          NSC\n346 -2.056537390  2.327647312                         NTAP\n347 -2.107553254 -2.272737433                         NTRS\n348  0.298397614  1.873479522                          NUE\n349 -2.749390836  2.240512106                         NVDA\n350 -0.514875167  1.792082462                          NWL\n351  0.324776721  0.886290341                          NWS\n352  0.355922062  0.896421285                         NWSA\n353  2.717408233 -6.041444659                            O\n354  3.910663563  1.442200320                          OKE\n355 -0.272444272 -1.224362354                          OMC\n356 -0.319352485  1.551813135                         ORCL\n357  0.343516721 -0.096170104                         ORLY\n358  3.997615700  1.277704377                          OXY\n359 -1.059917683  0.532571747                         PAYX\n360 -1.963944588 -2.719452539                         PBCT\n361  0.303638050  2.890098904                         PCAR\n362  3.184462320 -3.368547108                          PEG\n363  1.512753618 -2.267461814                          PEP\n364 -1.466890957 -0.109016945                          PFE\n365 -1.486797142 -2.152341016                          PFG\n366  1.577975706 -2.475750855                           PG\n367 -0.919336806 -0.770902222                          PGR\n368  0.183234454  2.829080707                           PH\n369  0.925176575  0.829062003                          PHM\n370  0.541477123  2.397525125                          PKG\n371 -1.156054380  2.019792494                          PKI\n372  2.557183987 -6.014472615                          PLD\n373  1.267104260 -2.296327348                           PM\n374 -2.198201480 -2.567533809                          PNC\n375  0.564327841  2.234253655                          PNR\n376  3.058537156 -3.362714593                          PNW\n377  0.602026885  1.686207055                          PPG\n378  3.016665610 -3.512479477                          PPL\n379 -1.218681966 -1.823868097                         PRGO\n380 -1.560098488 -2.205185558                          PRU\n381  2.818035117 -6.264690834                          PSA\n382  3.445926581  1.411114362                          PSX\n383 -0.057225499  1.419981139                          PVH\n384 -0.202192773  2.669516488                          PWR\n385  4.392198330  1.386510130                          PXD\n386 -2.442823893  2.445696668                         PYPL\n387 -0.004170424  2.089189986                         QCOM\n388 -1.781090171  3.486771873                         QRVO\n389 -1.429941914  1.875490636                          RCL\n390 -0.998737889 -1.169510045                           RE\n391  2.406870941 -6.297124256                          REG\n392 -1.762225239  0.634967099                         REGN\n393 -2.650340158 -2.665805446                           RF\n394 -1.973170690  1.968677310                          RHI\n395 -0.670861789  0.281217771                          RHT\n396 -2.543062256 -2.078413860                          RJF\n397  0.008451438  1.140443055                           RL\n398 -0.972979775  1.474757490                          RMD\n399  0.167376016  2.904560355                          ROK\n400 -0.288149646  0.472905851                          ROL\n401  0.418730609  2.400524071                          ROP\n402  0.944713829  0.048179316                         ROST\n403 -0.192717631 -0.563212587                          RSG\n404  1.087987881  3.237462950                          RTN\n405  2.564280190 -5.641389931                         SBAC\n406 -0.340073563  0.340765485                         SBUX\n407 -2.384224286 -2.111449936                         SCHW\n408 -0.083070870 -0.627519644                          SEE\n409  0.700955453  1.546147430                          SHW\n410 -2.604125639 -2.481622331                         SIVB\n411  1.775901671 -2.284581212                          SJM\n412  4.266742188  1.447873754                          SLB\n413  2.448611797 -6.365612665                          SLG\n414 -0.667601173  1.463353391                          SNA\n415 -2.113225491  2.466318012                         SNPS\n416  3.161312224 -3.392549145                           SO\n417  2.296068130 -6.178479086                          SPG\n418 -1.952340903  1.529696191                         SPGI\n419  2.919828835 -3.261315784                          SRE\n420 -2.458061063 -2.756259794                          STI\n421 -1.922375516 -2.392373026                          STT\n422 -1.084504638  3.146564503                          STX\n423 -0.619345133  0.293273411                          STZ\n424  0.074160388  2.881724006                          SWK\n425 -1.757257300  3.397224687                         SWKS\n426 -2.427124360 -1.947135313                          SYF\n427 -1.426253280  1.022573504                          SYK\n428  0.453111005  1.069279226                         SYMC\n429  0.092400227 -0.733697504                          SYY\n430  0.238179388 -1.556301752                            T\n431  0.976600520 -1.777834219                          TAP\n432 -0.107070602  2.509282839                          TDG\n433 -0.617108952  2.701702628                          TEL\n434 -1.335144848  1.144418344                          TFX\n435  0.957670429 -0.091450174                          TGT\n436  0.037851059  0.970790253                          TIF\n437  0.835204870  0.167226340                          TJX\n438 -1.447449647 -1.588446692                          TMK\n439 -0.944112996  1.863837266                          TMO\n440 -0.135245934  1.227007037                          TPR\n441 -2.353910679  2.354022587                         TRIP\n442 -2.300859915 -1.786098796                         TROW\n443 -0.793901543 -1.172948556                          TRV\n444  0.646277815  0.024900141                         TSCO\n445  1.659641619 -2.104594768                          TSN\n446 -2.000616070  2.004642837                          TSS\n447 -2.529233883  2.830185486                         TTWO\n448 -2.757210570  2.603516840                         TWTR\n449 -1.547210455  3.286185920                          TXN\n450  0.337044751  2.515424942                          TXT\n451  0.141576302  1.457551762                           UA\n452  0.121799411  1.285783811                          UAA\n453  1.399244971  1.450563635                          UAL\n454  2.490346363 -5.884979660                          UDR\n455  0.080845693 -0.199303819                          UHS\n456 -0.815302264  0.749992919                         ULTA\n457 -0.847029938  0.075046786                          UNH\n458 -1.453721863 -2.138051777                          UNM\n459 -0.428388132  2.166934629                          UNP\n460  0.054002972  0.429458777                          UPS\n461  0.332044036  2.822787786                          URI\n462 -1.263857448 -1.670292548                          USB\n463  0.071623149  2.238753378                          UTX\n464 -1.981422871  2.243242690                            V\n465 -0.869476578  1.894943435                          VAR\n466 -0.130111163  1.092356097                          VFC\n467 -0.124170671 -1.291849261                         VIAB\n468  3.335595058  1.383661944                          VLO\n469  0.544401745  1.041324768                          VMC\n470  2.331455206 -6.356966677                          VNO\n471 -0.601623471  0.826642959                         VRSK\n472 -2.440038276  2.422005092                         VRSN\n473 -1.850405157  2.161520301                         VRTX\n474  2.666841993 -6.310069614                          VTR\n475  0.852534598 -1.821016368                           VZ\n476 -0.904018094  2.097260212                          WAT\n477 -0.470635573 -0.420277271                          WBA\n478 -0.903294944  0.135610483                          WCG\n479 -1.136130493  3.192118747                          WDC\n480  3.199108906 -3.401740371                          WEC\n481  2.715131355 -6.484697752                         WELL\n482 -1.843789586 -2.468970543                          WFC\n483  0.466958406  1.453257602                          WHR\n484 -1.029736460 -0.848492919                         WLTW\n485 -0.005020159 -0.432425642                           WM\n486  3.880810204  1.530415157                          WMB\n487  0.750132381 -0.637103823                          WMT\n488  0.471508680  2.279547017                          WRK\n489 -0.416832853  0.267636408                           WU\n490  2.161817890 -6.327290007                           WY\n491 -0.239455118  2.556289675                         WYNN\n492  4.495804905  1.289852019                          XEC\n493  3.359146094 -3.229623657                          XEL\n494 -1.590623599  3.342234602                         XLNX\n495  3.956832065  1.398136965                          XOM\n496 -1.398707422  0.795174193                         XRAY\n497 -0.203824165  1.651709493                          XRX\n498  0.453645379  2.681064245                          XYL\n499 -0.313055710  0.097162980                          YUM\n500 -1.092526221  0.809159323                          ZBH\n501 -2.663933541 -2.642333729                         ZION\n502 -0.973227830  1.336913750                          ZTS\n\n# Output: umap_results_tbl\n\nFinally, let’s make a quick visualization of the umap_results_tbl.\n\nPipe the umap_results_tbl into ggplot() mapping the columns to x-axis and y-axis\nAdd a geom_point() geometry with an alpha = 0.5\nApply theme_tq() and add a title “UMAP Projection”\n\n\n# Visualize UMAP results\numap_results_tbl %&gt;%\n  ggplot(mapping = aes(x = V1, y = V2)) +\n  geom_point(alpha = 0.5) +\n  theme_tq() +\n  ggtitle(\"UMAP Projection\")\n\n\n\n\n\n\n\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "Chapter_1_Challenge_fahad.html#step-6---combine-k-means-and-umap",
    "href": "Chapter_1_Challenge_fahad.html#step-6---combine-k-means-and-umap",
    "title": "Challenge 1 - Data Science Fundamentals",
    "section": "Step 6 - Combine K-Means and UMAP",
    "text": "Step 6 - Combine K-Means and UMAP\nNext, we combine the K-Means clusters and the UMAP 2D representation\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl &lt;- read_rds(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/umap_results_tbl.rds\")\n\nFirst, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. Have a look at the business case to recall how that works.\n\n# Get the k_means_obj from the 10th center\nk_means_obj &lt;- k_means_mapped_tbl %&gt;%\n    pull(k_means) %&gt;%\n    pluck(10)\n# Store as k_means_obj\n\nNext, we’ll combine the clusters from the k_means_obj with the umap_results_tbl.\n\nBegin with the k_means_obj\nAugment the k_means_obj with the stock_date_matrix_tbl to get the clusters added to the end of the tibble\nSelect just the symbol and .cluster columns\nLeft join the result with the umap_results_tbl by the symbol column\nLeft join the result with the result of sp_500_index_tbl %&gt;% select(symbol, company, sector) by the symbol column.\nStore the output as umap_kmeans_results_tbl\n\n\n# Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;%\n  augment(stock_date_matrix_tbl) %&gt;%\n  select(symbol, .cluster) %&gt;%\n  left_join(umap_results_tbl) %&gt;%\n  left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector), by = \"symbol\")\n\nJoining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl\n\n# A tibble: 502 × 6\n   symbol .cluster      V1      V2 company                       sector         \n   &lt;chr&gt;  &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;          \n 1 A      7        -0.764   1.65   Agilent Technologies Inc.     Health Care    \n 2 AAL    2        -2.70    0.455  American Airlines Group Inc.  Industrials    \n 3 AAP    10        0.739  -0.0320 Advance Auto Parts Inc.       Consumer Discr…\n 4 AAPL   9         0.0130  3.09   Apple Inc.                    Information Te…\n 5 ABBV   7        -0.965  -0.0193 AbbVie Inc.                   Health Care    \n 6 ABC    5        -0.506  -0.659  AmerisourceBergen Corporation Health Care    \n 7 ABMD   9         0.436   3.10   ABIOMED Inc.                  Health Care    \n 8 ABT    7        -0.262   1.35   Abbott Laboratories           Health Care    \n 9 ACN    7         0.0598  1.63   Accenture Plc Class A         Information Te…\n10 ADBE   9         0.570   3.43   Adobe Inc.                    Information Te…\n# ℹ 492 more rows\n\n# Output: umap_kmeans_results_tbl \n\nPlot the K-Means and UMAP results.\n\nBegin with the umap_kmeans_results_tbl\nUse ggplot() mapping V1, V2 and color = .cluster\nAdd the geom_point() geometry with alpha = 0.5\nApply colors as you desire (e.g. scale_color_manual(values = palette_light() %&gt;% rep(3)))\n\n\n# Visualize the combined K-Means and UMAP results\numap_kmeans_results_tbl %&gt;%\n  ggplot(aes(x = V1, y = V2, color =.cluster)) +\n  geom_point(alpha = 0.5) \n\n\n\n\n\n\n\n\nCongratulations! You are done with the 1st challenge!"
  },
  {
    "objectID": "05_lime.html",
    "href": "05_lime.html",
    "title": "Challenge 5- LIME",
    "section": "",
    "text": "library(h2o)\nlibrary(recipes)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(lime)\nlibrary(rsample)\n\n\nprocess_hr_data_readable &lt;- function(data, definitions_tbl) {\n\n    definitions_list &lt;- definitions_tbl %&gt;%\n        fill(...1, .direction = \"down\") %&gt;%\n        filter(!is.na(...2)) %&gt;%\n        separate(...2, into = c(\"key\", \"value\"), sep = \" '\", remove = TRUE) %&gt;%\n        rename(column_name = ...1) %&gt;%\n        mutate(key = as.numeric(key)) %&gt;%\n        mutate(value = value %&gt;% str_replace(pattern = \"'\", replacement = \"\")) %&gt;%\n        split(.$column_name) %&gt;%\n        map(~ select(., -column_name)) %&gt;%\n        map(~ mutate(., value = as_factor(value))) \n    \n    for (i in seq_along(definitions_list)) {\n        list_name &lt;- names(definitions_list)[i]\n        colnames(definitions_list[[i]]) &lt;- c(list_name, paste0(list_name, \"_value\"))\n    }\n    \n    data_merged_tbl &lt;- list(HR_Data = data) %&gt;%\n        append(definitions_list, after = 1) %&gt;%\n        reduce(left_join) %&gt;%\n        select(-one_of(names(definitions_list))) %&gt;%\n        set_names(str_replace_all(names(.), pattern = \"_value\", \n                                            replacement = \"\")) %&gt;%\n        select(sort(names(.))) %&gt;%\n        mutate_if(is.character, as.factor) %&gt;%\n        mutate(\n            BusinessTravel = BusinessTravel %&gt;% fct_relevel(\"Non-Travel\", \n                                                            \"Travel_Rarely\", \n                                                            \"Travel_Frequently\"),\n            MaritalStatus  = MaritalStatus %&gt;% fct_relevel(\"Single\", \n                                                           \"Married\", \n                                                           \"Divorced\")\n        )\n    \n    return(data_merged_tbl)\n    \n}\n\n\nemployee_attrition_tbl &lt;- read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/03_ml_aut_files/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\")\ndefinitions_raw_tbl    &lt;- read_excel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/03_ml_aut_files/data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\nemployee_attrition_readable_tbl &lt;- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj &lt;- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nrecipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%\n                step_zv(all_predictors()) %&gt;%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %&gt;% \n                prep()\n\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\Rtmp6PurmV\\file2d507d7a45b9/h2o_Lenovo_started_from_r.out\n    C:\\Users\\Lenovo\\AppData\\Local\\Temp\\Rtmp6PurmV\\file2d506c827f47/h2o_Lenovo_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         2 seconds 771 milliseconds \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    6 months and 5 days \n    H2O cluster name:           H2O_started_from_R_Lenovo_drs368 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.74 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.0 (2023-04-21 ucrt) \n\n#automl_leader &lt;- h2o.loadModel(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/StackedEnsemble_BestOfFamily_2_AutoML_1_20220612_170511\")\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the target and predictors\ny &lt;- \"Attrition\"\nx &lt;- setdiff(names(train_h2o), y)\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5 \n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n11:20:43.696: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n11:20:43.725: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n\nautoml_leader &lt;- automl_models_h2o@leader\n\n\nexplainer &lt;- train_tbl %&gt;%\n    select(-Attrition) %&gt;%\n    lime(\n        model           = automl_leader,\n        bin_continuous  = TRUE,\n        n_bins          = 4,\n        quantile_bins   = TRUE\n    )\n\nexplainer\n\n$model\nModel Details:\n==============\n\nH2OBinomialModel: stackedensemble\nModel ID:  StackedEnsemble_AllModels_2_AutoML_1_20240626_112043 \nModel Summary for Stacked Ensemble: \n                                         key            value\n1                          Stacking strategy cross_validation\n2       Number of base models (used / total)              5/9\n3           # GBM base models (used / total)              3/5\n4  # DeepLearning base models (used / total)              1/1\n5           # GLM base models (used / total)              1/1\n6           # DRF base models (used / total)              0/2\n7                      Metalearner algorithm              GLM\n8         Metalearner fold assignment scheme           Random\n9                         Metalearner nfolds                5\n10                   Metalearner fold_column               NA\n11        Custom metalearner hyperparameters             None\n\n\nH2OBinomialMetrics: stackedensemble\n** Reported on training data. **\n\nMSE:  0.05375346\nRMSE:  0.2318479\nLogLoss:  0.2027985\nMean Per-Class Error:  0.1373576\nAUC:  0.9387487\nAUCPR:  0.8592039\nGini:  0.8774974\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error      Rate\nNo     888  21 0.023102   =21/909\nYes     39 116 0.251613   =39/155\nTotals 927 137 0.056391  =60/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.376327   0.794521 114\n2                       max f2  0.230400   0.803140 164\n3                 max f0point5  0.551058   0.856874  74\n4                 max accuracy  0.420139   0.944549 105\n5                max precision  0.975793   1.000000   0\n6                   max recall  0.006598   1.000000 388\n7              max specificity  0.975793   1.000000   0\n8             max absolute_mcc  0.420139   0.764506 105\n9   max min_per_class_accuracy  0.185924   0.878988 183\n10 max mean_per_class_accuracy  0.230400   0.887778 164\n11                     max tns  0.975793 909.000000   0\n12                     max fns  0.975793 154.000000   0\n13                     max fps  0.000731 909.000000 399\n14                     max tps  0.006598 155.000000 388\n15                     max tnr  0.975793   1.000000   0\n16                     max fnr  0.975793   0.993548   0\n17                     max fpr  0.000731   1.000000 399\n18                     max tpr  0.006598   1.000000 388\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: stackedensemble\n** Reported on validation data. **\n\nMSE:  0.1008602\nRMSE:  0.317585\nLogLoss:  0.3336465\nMean Per-Class Error:  0.1987111\nAUC:  0.868063\nAUCPR:  0.7193464\nGini:  0.736126\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error     Rate\nNo     135  12 0.081633  =12/147\nYes     12  26 0.315789   =12/38\nTotals 147  38 0.129730  =24/185\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.337626   0.684211  37\n2                       max f2  0.200157   0.714286  57\n3                 max f0point5  0.474533   0.708955  23\n4                 max accuracy  0.474533   0.870270  23\n5                max precision  0.940023   1.000000   0\n6                   max recall  0.017752   1.000000 145\n7              max specificity  0.940023   1.000000   0\n8             max absolute_mcc  0.337626   0.602578  37\n9   max min_per_class_accuracy  0.200157   0.789474  57\n10 max mean_per_class_accuracy  0.269144   0.807197  45\n11                     max tns  0.940023 147.000000   0\n12                     max fns  0.940023  37.000000   0\n13                     max fps  0.001591 147.000000 184\n14                     max tps  0.017752  38.000000 145\n15                     max tnr  0.940023   1.000000   0\n16                     max fnr  0.940023   0.973684   0\n17                     max fpr  0.001591   1.000000 184\n18                     max tpr  0.017752   1.000000 145\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: stackedensemble\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.08590584\nRMSE:  0.293097\nLogLoss:  0.3021878\nMean Per-Class Error:  0.2155967\nAUC:  0.8392562\nAUCPR:  0.6039713\nGini:  0.6785124\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     822  87 0.095710    =87/909\nYes     52 103 0.335484    =52/155\nTotals 874 190 0.130639  =139/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.253182   0.597101 144\n2                       max f2  0.165232   0.642002 201\n3                 max f0point5  0.396519   0.650741  90\n4                 max accuracy  0.396519   0.896617  90\n5                max precision  0.967443   1.000000   0\n6                   max recall  0.000598   1.000000 399\n7              max specificity  0.967443   1.000000   0\n8             max absolute_mcc  0.396519   0.540769  90\n9   max min_per_class_accuracy  0.146528   0.767742 216\n10 max mean_per_class_accuracy  0.244599   0.786454 149\n11                     max tns  0.967443 909.000000   0\n12                     max fns  0.967443 154.000000   0\n13                     max fps  0.000598 909.000000 399\n14                     max tps  0.000598 155.000000 399\n15                     max tnr  0.967443   1.000000   0\n16                     max fnr  0.967443   0.993548   0\n17                     max fpr  0.000598   1.000000 399\n18                     max tpr  0.000598   1.000000 399\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nCross-Validation Metrics Summary: \n               mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\naccuracy   0.897478 0.013204   0.876238   0.905983   0.901554   0.909548\nauc        0.837242 0.065571   0.800792   0.878349   0.852814   0.909991\nerr        0.102522 0.013204   0.123762   0.094017   0.098446   0.090452\nerr_count 21.800000 3.271085  25.000000  22.000000  19.000000  18.000000\nf0point5   0.632989 0.096357   0.590551   0.754717   0.666667   0.657143\n          cv_5_valid\naccuracy    0.894068\nauc         0.744265\nerr         0.105932\nerr_count  25.000000\nf0point5    0.495868\n\n---\n                        mean        sd cv_1_valid cv_2_valid cv_3_valid\nprecision           0.640836  0.097618   0.625000   0.761905   0.695652\nr2                  0.296269  0.099698   0.244036   0.388037   0.302603\nrecall              0.622885  0.162566   0.483871   0.727273   0.571429\nresidual_deviance 127.125380 20.503176 134.964460 146.976350 113.411110\nrmse                0.292640  0.018314   0.313385   0.305668   0.294106\nspecificity         0.942809  0.014539   0.947368   0.947368   0.957576\n                  cv_4_valid cv_5_valid\nprecision           0.621622   0.500000\nr2                  0.390633   0.156038\nrecall              0.851852   0.480000\nresidual_deviance  98.454940 141.820050\nrmse                0.267321   0.282723\nspecificity         0.918605   0.943128\n\n$preprocess\nfunction (x) \nx\n&lt;bytecode: 0x000001e6c0fee200&gt;\n&lt;environment: 0x000001e6c0ff6518&gt;\n\n$bin_continuous\n[1] TRUE\n\n$n_bins\n[1] 4\n\n$quantile_bins\n[1] TRUE\n\n$use_density\n[1] TRUE\n\n$feature_type\n                     Age           BusinessTravel                DailyRate \n               \"numeric\"                 \"factor\"                \"numeric\" \n              Department         DistanceFromHome                Education \n                \"factor\"                \"numeric\"                 \"factor\" \n          EducationField           EmployeeNumber  EnvironmentSatisfaction \n                \"factor\"                \"numeric\"                 \"factor\" \n                  Gender               HourlyRate           JobInvolvement \n                \"factor\"                \"numeric\"                 \"factor\" \n                JobLevel                  JobRole          JobSatisfaction \n                \"factor\"                 \"factor\"                 \"factor\" \n           MaritalStatus            MonthlyIncome              MonthlyRate \n                \"factor\"                \"numeric\"                \"numeric\" \n      NumCompaniesWorked                 OverTime        PercentSalaryHike \n               \"numeric\"                 \"factor\"                \"numeric\" \n       PerformanceRating RelationshipSatisfaction         StockOptionLevel \n                \"factor\"                 \"factor\"                 \"factor\" \n       TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance \n               \"numeric\"                \"numeric\"                 \"factor\" \n          YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion \n               \"numeric\"                \"numeric\"                \"numeric\" \n    YearsWithCurrManager \n               \"numeric\" \n\n$bin_cuts\n$bin_cuts$Age\n  0%  25%  50%  75% 100% \n  18   30   36   43   60 \n\n$bin_cuts$BusinessTravel\nNULL\n\n$bin_cuts$DailyRate\n  0%  25%  50%  75% 100% \n 102  465  797 1147 1499 \n\n$bin_cuts$Department\nNULL\n\n$bin_cuts$DistanceFromHome\n  0%  25%  50%  75% 100% \n   1    2    7   14   29 \n\n$bin_cuts$Education\nNULL\n\n$bin_cuts$EducationField\nNULL\n\n$bin_cuts$EmployeeNumber\n  0%  25%  50%  75% 100% \n   1  511 1040 1573 2065 \n\n$bin_cuts$EnvironmentSatisfaction\nNULL\n\n$bin_cuts$Gender\nNULL\n\n$bin_cuts$HourlyRate\n  0%  25%  50%  75% 100% \n  30   49   66   83  100 \n\n$bin_cuts$JobInvolvement\nNULL\n\n$bin_cuts$JobLevel\nNULL\n\n$bin_cuts$JobRole\nNULL\n\n$bin_cuts$JobSatisfaction\nNULL\n\n$bin_cuts$MaritalStatus\nNULL\n\n$bin_cuts$MonthlyIncome\n   0%   25%   50%   75%  100% \n 1051  2929  4908  8474 19999 \n\n$bin_cuts$MonthlyRate\n   0%   25%   50%   75%  100% \n 2094  8423 14470 20689 26968 \n\n$bin_cuts$NumCompaniesWorked\n  0%  25%  50%  75% 100% \n   0    1    2    4    9 \n\n$bin_cuts$OverTime\nNULL\n\n$bin_cuts$PercentSalaryHike\n  0%  25%  50%  75% 100% \n  11   12   14   18   25 \n\n$bin_cuts$PerformanceRating\nNULL\n\n$bin_cuts$RelationshipSatisfaction\nNULL\n\n$bin_cuts$StockOptionLevel\nNULL\n\n$bin_cuts$TotalWorkingYears\n  0%  25%  50%  75% 100% \n   0    6   10   15   38 \n\n$bin_cuts$TrainingTimesLastYear\n  0%  25%  50% 100% \n   0    2    3    6 \n\n$bin_cuts$WorkLifeBalance\nNULL\n\n$bin_cuts$YearsAtCompany\n  0%  25%  50%  75% 100% \n   0    3    5    9   37 \n\n$bin_cuts$YearsInCurrentRole\n  0%  25%  50%  75% 100% \n   0    2    3    7   18 \n\n$bin_cuts$YearsSinceLastPromotion\n  0%  50%  75% 100% \n   0    1    2   15 \n\n$bin_cuts$YearsWithCurrManager\n  0%  25%  50%  75% 100% \n   0    2    3    7   17 \n\n\n$feature_distribution\n$feature_distribution$Age\n\n        1         2         3         4 \n0.2602082 0.2834267 0.2217774 0.2345877 \n\n$feature_distribution$BusinessTravel\n\n       Non-Travel     Travel_Rarely Travel_Frequently \n        0.1000801         0.7181745         0.1817454 \n\n$feature_distribution$DailyRate\n\n        1         2         3         4 \n0.2514011 0.2489992 0.2497998 0.2497998 \n\n$feature_distribution$Department\n\n       Human Resources Research & Development                  Sales \n            0.04323459             0.65092074             0.30584468 \n\n$feature_distribution$DistanceFromHome\n\n        1         2         3         4 \n0.2954363 0.2369896 0.2241793 0.2433947 \n\n$feature_distribution$Education\n\nBelow College       College      Bachelor        Master        Doctor \n   0.11689351    0.18895116    0.38510809    0.27461970    0.03442754 \n\n$feature_distribution$EducationField\n\n Human Resources    Life Sciences        Marketing          Medical \n      0.01761409       0.41793435       0.10888711       0.31144916 \n           Other Technical Degree \n      0.05444355       0.08967174 \n\n$feature_distribution$EmployeeNumber\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$EnvironmentSatisfaction\n\n      Low    Medium      High Very High \n0.1913531 0.1961569 0.3018415 0.3106485 \n\n$feature_distribution$Gender\n\n   Female      Male \n0.4123299 0.5876701 \n\n$feature_distribution$HourlyRate\n\n        1         2         3         4 \n0.2618094 0.2473979 0.2449960 0.2457966 \n\n$feature_distribution$JobInvolvement\n\n       Low     Medium       High  Very High \n0.05684548 0.25780624 0.58927142 0.09607686 \n\n$feature_distribution$JobLevel\n\n         1          2          3          4          5 \n0.36829464 0.36509207 0.14651721 0.07526021 0.04483587 \n\n$feature_distribution$JobRole\n\nHealthcare Representative           Human Resources     Laboratory Technician \n               0.08646918                0.03682946                0.18174540 \n                  Manager    Manufacturing Director         Research Director \n               0.06885508                0.09927942                0.05924740 \n       Research Scientist           Sales Executive      Sales Representative \n               0.18654924                0.22337870                0.05764612 \n\n$feature_distribution$JobSatisfaction\n\n      Low    Medium      High Very High \n0.1873499 0.1985588 0.3018415 0.3122498 \n\n$feature_distribution$MaritalStatus\n\n   Single   Married  Divorced \n0.3306645 0.4571657 0.2121697 \n\n$feature_distribution$MonthlyIncome\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$MonthlyRate\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$NumCompaniesWorked\n\n         1          2          3          4 \n0.48118495 0.09927942 0.20496397 0.21457166 \n\n$feature_distribution$OverTime\n\n       No       Yes \n0.7165733 0.2834267 \n\n$feature_distribution$PercentSalaryHike\n\n        1         2         3         4 \n0.2866293 0.2738191 0.2289832 0.2105685 \n\n$feature_distribution$PerformanceRating\n\n        Low        Good   Excellent Outstanding \n  0.0000000   0.0000000   0.8414732   0.1585268 \n\n$feature_distribution$RelationshipSatisfaction\n\n      Low    Medium      High Very High \n0.1889512 0.2161729 0.3018415 0.2930344 \n\n$feature_distribution$StockOptionLevel\n\n         0          1          2          3 \n0.43554844 0.40592474 0.10168135 0.05684548 \n\n$feature_distribution$TotalWorkingYears\n\n        1         2         3         4 \n0.3050440 0.3306645 0.1224980 0.2417934 \n\n$feature_distribution$TrainingTimesLastYear\n\n        1         2         3 \n0.4603683 0.3306645 0.2089672 \n\n$feature_distribution$WorkLifeBalance\n\n       Bad       Good     Better       Best \n0.05204163 0.22497998 0.61889512 0.10408327 \n\n$feature_distribution$YearsAtCompany\n\n        1         2         3         4 \n0.3226581 0.2137710 0.2217774 0.2417934 \n\n$feature_distribution$YearsInCurrentRole\n\n         1          2          3          4 \n0.46757406 0.08726982 0.27542034 0.16973579 \n\n$feature_distribution$YearsSinceLastPromotion\n\n        1         2         3 \n0.6413131 0.1120897 0.2465973 \n\n$feature_distribution$YearsWithCurrManager\n\n         1          2          3          4 \n0.46357086 0.09767814 0.25300240 0.18574860 \n\n\nattr(,\"class\")\n[1] \"data_frame_explainer\" \"explainer\"            \"list\"                \n\n\n\nexplanation &lt;- test_tbl %&gt;%\n    slice(1:20) %&gt;%\n    select(-Attrition) %&gt;%\n    lime::explain(\n    \n        # Pass our explainer object\n        explainer = explainer,\n        # Because it is a binary classification model: 1\n        n_labels   = 1,\n        # number of features to be returned\n        n_features = 8,\n        # number of localized linear models\n        n_permutations = 5000,\n        # Let's start with 1\n        kernel_width   = 1\n    )\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation\n\n# A tibble: 160 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 classificat… 1     No         0.565    0.345           0.907            0.517\n 2 classificat… 1     No         0.565    0.345           0.907            0.517\n 3 classificat… 1     No         0.565    0.345           0.907            0.517\n 4 classificat… 1     No         0.565    0.345           0.907            0.517\n 5 classificat… 1     No         0.565    0.345           0.907            0.517\n 6 classificat… 1     No         0.565    0.345           0.907            0.517\n 7 classificat… 1     No         0.565    0.345           0.907            0.517\n 8 classificat… 1     No         0.565    0.345           0.907            0.517\n 9 classificat… 2     No         0.777    0.420           0.737            0.620\n10 classificat… 2     No         0.777    0.420           0.737            0.620\n# ℹ 150 more rows\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\n\nexplanation %&gt;% \n  as.tibble()\n\n# A tibble: 160 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 classificat… 1     No         0.565    0.345           0.907            0.517\n 2 classificat… 1     No         0.565    0.345           0.907            0.517\n 3 classificat… 1     No         0.565    0.345           0.907            0.517\n 4 classificat… 1     No         0.565    0.345           0.907            0.517\n 5 classificat… 1     No         0.565    0.345           0.907            0.517\n 6 classificat… 1     No         0.565    0.345           0.907            0.517\n 7 classificat… 1     No         0.565    0.345           0.907            0.517\n 8 classificat… 1     No         0.565    0.345           0.907            0.517\n 9 classificat… 2     No         0.777    0.420           0.737            0.620\n10 classificat… 2     No         0.777    0.420           0.737            0.620\n# ℹ 150 more rows\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\ncase_1 &lt;- explanation %&gt;%\n    filter(case == 1)\n\ncase_1 %&gt;%\n    plot_features()\n\n\n\n\n\n\n\ncase_1 %&gt;%\n  ggplot(aes(y=feature_desc, x =feature_weight)) +\n  geom_col(aes(fill = feature_weight &gt; 0)) +\n  xlab(\"Weight\") + \n  ylab(\"Feature\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Contradicts\", \"Supports\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nexplanation %&gt;%\n  mutate(case = as.double(case)) %&gt;%\n  ggplot(aes(y=feature_desc, x =case, fill = feature_weight)) +\n  geom_tile() +\n  facet_wrap(~label)"
  },
  {
    "objectID": "06_dl.html",
    "href": "06_dl.html",
    "title": "Challenge 6 Deep Learning",
    "section": "",
    "text": "library(tidyverse)\nlibrary(keras)\nlibrary(lime)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(corrr)\n\n\nchurn_data_raw &lt;- read.csv(\"D:/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\nglimpse(churn_data_raw)\n\nRows: 7,043\nColumns: 21\n$ customerID       &lt;chr&gt; \"7590-VHVEG\", \"5575-GNVDE\", \"3668-QPYBK\", \"7795-CFOCW…\n$ gender           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Female\",…\n$ SeniorCitizen    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Partner          &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes…\n$ Dependents       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\"…\n$ tenure           &lt;int&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2…\n$ PhoneService     &lt;chr&gt; \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ MultipleLines    &lt;chr&gt; \"No phone service\", \"No\", \"No\", \"No phone service\", \"…\n$ InternetService  &lt;chr&gt; \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"Fiber opt…\n$ OnlineSecurity   &lt;chr&gt; \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"…\n$ OnlineBackup     &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N…\n$ DeviceProtection &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Y…\n$ TechSupport      &lt;chr&gt; \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Yes…\n$ StreamingTV      &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Ye…\n$ StreamingMovies  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes…\n$ Contract         &lt;chr&gt; \"Month-to-month\", \"One year\", \"Month-to-month\", \"One …\n$ PaperlessBilling &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ PaymentMethod    &lt;chr&gt; \"Electronic check\", \"Mailed check\", \"Mailed check\", \"…\n$ MonthlyCharges   &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7…\n$ TotalCharges     &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949…\n$ Churn            &lt;chr&gt; \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Y…\n\n\n\nchurn_data_tbl &lt;- churn_data_raw %&gt;%\n                  select(Churn, everything(), -customerID) %&gt;%\n                  tidyr::drop_na()\n\n\n# Split test/training sets\nset.seed(100)\ntrain_test_split &lt;- rsample::initial_split(churn_data_tbl, prop =0.8)\ntrain_test_split\n\n&lt;Training/Testing/Total&gt;\n&lt;5625/1407/7032&gt;\n\n## &lt;Analysis/Assess/Total&gt;\n## &lt;5626/1406/7032&gt;\n\n# Retrieve train and test sets\ntrain_tbl &lt;- training(train_test_split)\ntest_tbl  &lt;- testing(train_test_split)\n\n\nchurn_data_tbl %&gt;% ggplot(aes(x = tenure)) + \n                     geom_histogram(binwidth = 0.5, fill =  \"#2DC6D6\") +\n                     labs(\n                       title = \"Tenure Counts Without Binning\",\n                       x     = \"tenure (month)\"\n                       )\n\n\n\n\n\n\n\n\n\nchurn_data_tbl %&gt;% ggplot(aes(x = tenure)) + \n  geom_histogram(bins = 6, color = \"white\", fill =  \"#2DC6D6\") +\n  labs(\n    title = \"Tenure Counts With Six Bins\",\n    x     = \"tenure (month)\"\n  )\n\n\n\n\n\n\n\n\n\nchurn_data_tbl %&gt;% ggplot(aes(x = TotalCharges)) + \n                     geom_histogram(bins = 100, fill =  \"#2DC6D6\") +\n                     labs(\n                       title = \"TotalCharges Histogram, 100 bins\",\n                       x     = \"TotalCharges\"\n                       )\n\n\n\n\n\n\n\n\n\nchurn_data_tbl_mod &lt;- churn_data_tbl %&gt;% \n  mutate(TotalCharges = log10(TotalCharges))\nchurn_data_tbl_mod %&gt;% ggplot(aes(x = TotalCharges)) + \n                     geom_histogram(bins = 100, fill =  \"#2DC6D6\") +\n                     labs(\n                       title = \"TotalCharges Histogram, 100 bins\",\n                       x     = \"TotalCharges\"\n                       )\n\n\n\n\n\n\n\n\n\n# Determine if log transformation improves correlation \n# between TotalCharges and Churn\n\ntrain_tbl %&gt;%\n    select(Churn, TotalCharges) %&gt;%\n    mutate(\n        Churn = Churn %&gt;% as.factor() %&gt;% as.numeric(),\n        LogTotalCharges = log(TotalCharges)\n        ) %&gt;%\n    correlate() %&gt;%\n    focus(Churn) %&gt;%\n    fashion()\n\n             term Churn\n1    TotalCharges  -.21\n2 LogTotalCharges  -.25\n\n\n\nchurn_data_tbl %&gt;% \n        pivot_longer(cols      = c(Contract, InternetService, MultipleLines, PaymentMethod), \n                     names_to  = \"feature\", \n                     values_to = \"category\") %&gt;% \n        ggplot(aes(category)) +\n          geom_bar(fill = \"#2DC6D6\") +\n          facet_wrap(~ feature, scales = \"free\") +\n          labs(\n            title = \"Features with multiple categories: Need to be one-hot encoded\"\n          ) +\n          theme(axis.text.x = element_text(angle = 25, \n                                           hjust = 1))\n\n\n\n\n\n\n\n\n\n# Create recipe\nrec_obj &lt;- recipe(Churn ~ ., data = train_tbl) %&gt;%\n    step_rm(Churn) %&gt;% \n    step_discretize(tenure, options = list(cuts = 6)) %&gt;%\n    step_log(TotalCharges) %&gt;%\n    step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %&gt;%\n    step_center(all_predictors(), -all_outcomes()) %&gt;%\n    step_scale(all_predictors(), -all_outcomes()) %&gt;%\n    prep(data = train_tbl)\n\n\nx_train_tbl &lt;- bake( rec_obj , new_data =  train_tbl)\nx_test_tbl  &lt;- bake( rec_obj , new_data =  test_tbl)\n\n\ny_train_vec &lt;- ifelse( train_tbl$Churn == \"Yes\", TRUE, FALSE )\ny_test_vec  &lt;- ifelse( test_tbl$Churn  == \"Yes\", TRUE, FALSE)\n\n\n# Building our Artificial Neural Network\n#model_keras &lt;- keras_model_sequential()\n\n#model_keras %&gt;% \n    # First hidden layer\n    #layer_dense(\n        #units              = 16, \n        #kernel_initializer = \"uniform\", \n        #activation         = \"relu\",\n        #input_shape        = ncol(x_train_tbl))%&gt;% \n    # Dropout to prevent overfitting\n    #layer_dropout(rate = 0.1) %&gt;%\n    # Second hidden layer\n    #layer_dense(\n        #units              = 16, \n        #kernel_initializer = \"uniform\", \n        #activation         = \"relu\") %&gt;% \n    # Dropout to prevent overfitting\n    #layer_dropout(rate = 0.1) %&gt;%\n    # Output layer\n    #layer_dense(\n        #units              = 1, \n        #kernel_initializer = \"uniform\", \n        #activation         = \"sigmoid\") %&gt;% \n    # Compile ANN\n    #Compile(\n        #optimizer = 'adam',\n        #loss      = 'binary_crossentropy',\n        #metrics   = c('accuracy')\n    #)\n#model_keras\n\nI tried to do the rest of the code but I do not know if it is correct. I commented the code out in order to make it readable.\n\n# x_train_mrx = as.matrix(x_train_tbl)\n# \n# ncol(x_train_tbl)\n# \n# fit_keras &lt;- keras::fit(\n#     object = model_keras,\n#     x = x_train_tbl, \n#     y = y_train_vec , \n#     epochs = 35 , \n#     batch_size = 50 ,\n#     validation_split = 0.3 \n#     )\n# \n# fit_keras\n# \n# plot(fit_keras) +\n#   labs(title = \"Deep Learning Training Results\") +\n#   theme(legend.position  = \"bottom\", \n#         strip.placement  = \"inside\",\n#         strip.background = element_rect(fill = \"#grey\"))\n# \n# # Predicted Class\n# yhat_keras_class_vec &lt;- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %&gt;%\n#     as.vector()\n# \n# # Predicted Class Probability\n# yhat_keras_prob_vec  &lt;- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %&gt;%\n#     as.vector()\n# \n# # Format test data and predictions for yardstick metrics\n# estimates_keras_tbl &lt;- tibble(\n#     truth      = as.factor(y_test_vec) %&gt;% fct_recode(yes = \"1\", no = \"0\"),\n#     estimate   = as.factor(yhat_keras_class_vec) %&gt;% fct_recode(yes = \"1\", no = \"0\"),\n#     class_prob = yhat_keras_prob_vec\n# )\n# \n# estimates_keras_tbl\n# \n# # Confusion Table\n# estimates_keras_tbl %&gt;% conf_mat(\n#   truth,\n#   estimate)\n# \n# # Accuracy\n# estimates_keras_tbl %&gt;% accuracy(truth, estimate)\n# \n# # AUC\n# estimates_keras_tbl %&gt;% roc_auc(\n#   data,\n#   truth,\n#   event_level = \"second\")\n# \n# # Precision\n# tibble(\n#     precision = precision(\n#                         data,\n#                         truth),\n#     recall    = recall(\n#                       data,\n#                       truth)\n# )\n# \n# # F1-Statistic\n# estimates_keras_tbl %&gt;% f_meas(truth, estimate, beta = 1)\n# \n# class(model_keras)\n# \n# # Setup lime::model_type() function for keras\n# model_type.keras.engine.sequential.Sequential  &lt;- function(x, ...) {\n#     return(\"classification\")\n# }\n# \n# # Setup lime::predict_model() function for keras\n# predict_model.keras.engine.sequential.Sequential &lt;- function(x, newdata, type, ...) {\n#     pred &lt;- predict_proba(object = x, x = as.matrix(newdata))\n#     return(data.frame(Yes = pred, No = 1 - pred))\n# }\n# \n# library(lime)\n# # Test our predict_model() function\n# predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %&gt;%\n#     tibble::as_tibble()\n# \n# # Run lime() on training set\n# explainer &lt;- lime::lime(\n#     x_train_tbl, \n#     y_train_vec , \n#     bin_continuous = FALSE)\n# \n# explanation &lt;- lime::explain(\n#     x_test_tbl[1:10,], \n#     explainer = explainer, \n#     n_labels   = 1, \n#     n_features = 51,\n#     kernel_width   = 1)\n# \n# # Feature correlations to Churn\n# corrr_analysis &lt;- x_train_tbl %&gt;%\n#     mutate(Churn = y_train_vec) %&gt;%\n#     correlate() %&gt;%\n#     focus(Churn) %&gt;%\n#     rename(feature = rowname) %&gt;%\n#     arrange(abs(Churn)) %&gt;%\n#     mutate(feature = as_factor(feature)) \n# corrr_analysis\n# \n# # Correlation visualization\n# corrr_analysis %&gt;%\n#   ggplot(aes(x = ..., y = fct_reorder(..., desc(...)))) +\n#   geom_point() +\n#   \n#   # Positive Correlations - Contribute to churn\n#   geom_segment(aes(xend = ..., yend = ...), \n#                color = \"red\", \n#                data = corrr_analysis %&gt;% filter(... &gt; ...)) +\n#   geom_point(color = \"red\", \n#              data = corrr_analysis %&gt;% filter(... &gt; ...)) +\n#   \n#   # Negative Correlations - Prevent churn\n#   geom_segment(aes(xend = 0, yend = feature), \n#                color = \"#2DC6D6\", \n#                data = ...) +\n#   geom_point(color = \"#2DC6D6\", \n#              data = ...) +\n#   \n#   # Vertical lines\n#   geom_vline(xintercept = 0, color = \"#f1fa8c\", size = 1, linetype = 2) +\n#   geom_vline( ... ) +\n#   geom_vline( ... ) +\n#   \n#   # Aesthetics\n#   labs( ... )"
  },
  {
    "objectID": "Chapter_2_Challenge-Fahad.html",
    "href": "Chapter_2_Challenge-Fahad.html",
    "title": "Challenge 2 - Supervised ML",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\n\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\")\n\nbike_orderlines_tbl &lt;- readRDS(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/bike_orderlines.rds\")\nbike_orderlines_tbl %&gt;%\nglimpse()\n\nRows: 15,644\nColumns: 18\n$ order_id       &lt;dbl&gt; 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n$ order_line     &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n$ order_date     &lt;dttm&gt; 2015-01-07, 2015-01-07, 2015-01-10, 2015-01-10, 2015-0…\n$ model          &lt;chr&gt; \"Spectral CF 7 WMN\", \"Ultimate CF SLX Disc 8.0 ETAP\", \"…\n$ model_year     &lt;dbl&gt; 2021, 2020, 2021, 2019, 2020, 2020, 2020, 2021, 2020, 2…\n$ category_1     &lt;chr&gt; \"Mountain\", \"Road\", \"Mountain\", \"Road\", \"Mountain\", \"Hy…\n$ category_2     &lt;chr&gt; \"Trail\", \"Race\", \"Trail\", \"Triathlon Bike\", \"Dirt Jump\"…\n$ category_3     &lt;chr&gt; \"Spectral\", \"Ultimate\", \"Neuron\", \"Speedmax\", \"Stitched…\n$ price          &lt;dbl&gt; 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n$ quantity       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n$ total_price    &lt;dbl&gt; 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n$ frame_material &lt;chr&gt; \"carbon\", \"carbon\", \"carbon\", \"carbon\", \"aluminium\", \"c…\n$ weight         &lt;dbl&gt; 13.80, 7.44, 14.06, 8.80, 11.50, 8.80, 8.20, 8.85, 14.4…\n$ url            &lt;chr&gt; \"https://www.canyon.com/en-de/mountain-bikes/trail-bike…\n$ bikeshop       &lt;chr&gt; \"AlexandeRad\", \"AlexandeRad\", \"WITT-RAD\", \"WITT-RAD\", \"…\n$ location       &lt;chr&gt; \"Hamburg, Hamburg\", \"Hamburg, Hamburg\", \"Bremen, Bremen…\n$ lat            &lt;dbl&gt; 53.57532, 53.57532, 53.07379, 53.07379, 48.78234, 48.78…\n$ lng            &lt;dbl&gt; 10.015340, 10.015340, 8.826754, 8.826754, 9.180819, 9.1…\n\nbike_features_tbl &lt;- readRDS(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/bike_features_tbl.rds\")\n\nbike_features_tbl &lt;- bike_features_tbl %&gt;%\n  select(model:url, `Rear Derailleur`, `Shift Lever`) %&gt;%\n  set_names(str_replace_all(names(.), \" |-\", \"_\"))\n\n\nset.seed(seed = 1113)\nbike_split_obj &lt;- initial_split(bike_features_tbl, strata = category_2)\n\ntrain_tbl &lt;- training(bike_split_obj)\ntest_tbl &lt;- testing(bike_split_obj)\n\nrecipe_obj &lt;- recipe(price ~ ., data = train_tbl) %&gt;%\n  step_rm(url) %&gt;%\n  step_dummy(all_nominal(), one_hot = TRUE)\n  \n\nbikes_wflow &lt;- workflow() %&gt;%\n  add_model(model_01_linear_lm_simple) %&gt;%\n  add_recipe(recipe_obj)\n\nbikes_fit &lt;- bikes_wflow %&gt;%\n  fit(data = train_tbl)\ncalc_metrics &lt;- function(model, new_data = test_tbl) {\n  model %&gt;%\n    predict(new_data = new_data) %&gt;%\n    bind_cols(new_data %&gt;% select(price)) %&gt;%\n    yardstick::metrics(truth = price, estimate = .pred)\n}\n\nbikes_fit %&gt;%\n  calc_metrics()\n\nWarning: ! There are new levels in a factor: `Aeroad CF SL Disc 8.0 Di2`, `Aeroad CF SL\n  7.0`, `Aeroad WMN CF SL 8.0`, `Endurace WMN AL Disc 7.0`, `Endurace WMN AL\n  Disc 6.0`, `Endurace CF SLX Disc 9.0 Di2`, `Endurace CF SL Disc 8.0 Pro`,\n  `Endurace CF SL Disc 8.0`, `Endurace WMN CF SL Disc 8.0`, `Endurace CF SL\n  Disc 7.0`, `Endurace CF SL Disc 8.0 Di2`, `Endurace:ON 7.0`, `Inflite CF SL\n  8`, `Speedmax CF SLX 8.0`, `Speedmax WMN CF SLX 8.0 SL`, `Speedmax WMN CF\n  8.0`, `Speedmax CF 8.0 LTD`, `Speedmax CF SLX 8.0 SL`, …, `Roadlite WMN CF\n  7.0`, and `Roadlite CF 7.0`.\n\n\nWarning: ! There are new levels in a factor: `E-Road`.\n\n\nWarning: ! There are new levels in a factor: `Endurace:ON`.\n\n\nWarning: ! There are new levels in a factor: `Campagnolo Potenza 11 GS`, `SRAM Force 1\n  GS`, and `SRAM X01 DH`.\n\n\nWarning: ! There are new levels in a factor: `Shimano XTR M9100 12s`, `Shimano XT\n  M8100`, and `SRAM Apex 11s`.\n\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\n\nWarning: A correlation computation is required, but the inputs are size zero or one and\nthe standard deviation cannot be computed. `NA` will be returned.\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard         NaN\n2 rsq     standard          NA\n3 mae     standard         NaN"
  },
  {
    "objectID": "Chapter_3_Challenge_fahad.html",
    "href": "Chapter_3_Challenge_fahad.html",
    "title": "CHallenge 3 Automated Machine Learning with H20",
    "section": "",
    "text": "First\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\nemployee_attrition_tbl &lt;- read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/daqtascience/ss24-bdml-Fahad221999/Business Decisions with Machine Learning/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\")\n\nRows: 1470 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\ndbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nemployee_attrition_tbl %&gt;%\n  glimpse()\n\nRows: 1,470\nColumns: 35\n$ Age                      &lt;dbl&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35, 2…\n$ Attrition                &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"…\n$ BusinessTravel           &lt;chr&gt; \"Travel_Rarely\", \"Travel_Frequently\", \"Travel…\n$ DailyRate                &lt;dbl&gt; 1102, 279, 1373, 1392, 591, 1005, 1324, 1358,…\n$ Department               &lt;chr&gt; \"Sales\", \"Research & Development\", \"Research …\n$ DistanceFromHome         &lt;dbl&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 26, …\n$ Education                &lt;dbl&gt; 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, 2, 3, …\n$ EducationField           &lt;chr&gt; \"Life Sciences\", \"Life Sciences\", \"Other\", \"L…\n$ EmployeeCount            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ EmployeeNumber           &lt;dbl&gt; 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16,…\n$ EnvironmentSatisfaction  &lt;dbl&gt; 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, 2, 3, …\n$ Gender                   &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Female\", \"Male\", \"…\n$ HourlyRate               &lt;dbl&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84, 4…\n$ JobInvolvement           &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, 3, 2, …\n$ JobLevel                 &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, …\n$ JobRole                  &lt;chr&gt; \"Sales Executive\", \"Research Scientist\", \"Lab…\n$ JobSatisfaction          &lt;dbl&gt; 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, 4, 3, …\n$ MaritalStatus            &lt;chr&gt; \"Single\", \"Married\", \"Single\", \"Married\", \"Ma…\n$ MonthlyIncome            &lt;dbl&gt; 5993, 5130, 2090, 2909, 3468, 3068, 2670, 269…\n$ MonthlyRate              &lt;dbl&gt; 19479, 24907, 2396, 23159, 16632, 11864, 9964…\n$ NumCompaniesWorked       &lt;dbl&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, 5, …\n$ Over18                   &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", …\n$ OverTime                 &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\",…\n$ PercentSalaryHike        &lt;dbl&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13, 1…\n$ PerformanceRating        &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, …\n$ RelationshipSatisfaction &lt;dbl&gt; 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, 3, 2, …\n$ StandardHours            &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8…\n$ StockOptionLevel         &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0, …\n$ TotalWorkingYears        &lt;dbl&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5, 3…\n$ TrainingTimesLastYear    &lt;dbl&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, 4, …\n$ WorkLifeBalance          &lt;dbl&gt; 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, …\n$ YearsAtCompany           &lt;dbl&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2, 4,…\n$ YearsInCurrentRole       &lt;dbl&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, 2, …\n$ YearsSinceLastPromotion  &lt;dbl&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, 0, …\n$ YearsWithCurrManager     &lt;dbl&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, 3, …\n\n\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n    \n    color_expr &lt;- enquo(color)\n    \n    if (rlang::quo_is_null(color_expr)) {\n        \n        g &lt;- data %&gt;%\n            ggpairs(lower = \"blank\") \n        \n    } else {\n        \n        color_name &lt;- quo_name(color_expr)\n        \n        g &lt;- data %&gt;%\n            ggpairs(mapping = aes_string(color = color_name), \n                    lower = \"blank\", legend = 1,\n                    diag = list(continuous = wrap(\"densityDiag\", \n                                                  alpha = density_alpha))) +\n            theme(legend.position = \"bottom\")\n    }\n    \n    return(g)\n    \n}\n\n\nplot &lt;- employee_attrition_tbl %&gt;%\n    select(Attrition, MonthlyIncome, PercentSalaryHike, StockOptionLevel, EnvironmentSatisfaction, WorkLifeBalance, JobInvolvement, OverTime, TrainingTimesLastYear, YearsAtCompany, YearsSinceLastPromotion) %&gt;%\n  plot_ggpairs(color = Attrition)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n  ggsave(\"asd.png\",plot, width = 15, height = 15)\n  plot\n\n\n\n\n\n\n\n\nIMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\n\nCompensation Features What can you deduce about the interaction between Monthly Income and Attrition?\n\n\nThose that are leaving have a lower Monthly Income\n\n\nCompensation Features What can you deduce about the interaction between Percent Salary Hike and Attrition?\n\n\nIt’s difficult to deduce anything based on the visualization\n\n\nCompensation Features What can you deduce about the interaction between Stock Option Level and Attrition?\n\n\nThose that are staying have a higher stock option level\n\n\nSurvey Results What can you deduce about the interaction between Environment Satisfaction and Attrition?\n\n\nA higher proportion of those leaving have a low environment satisfaction level\n\n\nSurvey Results What can you deduce about the interaction between Work Life Balance and Attrition\n\n\nThose that are staying have a higher density of 2’s and 3’\n\n\nPerformance Data What Can you deduce about the interaction between Job Involvement and Attrition?\n\n\nThose that are leaving have a lower density of 3’s and 4’s\n\n\nWork-Life Features What can you deduce about the interaction between Over Time and Attrition? a The proportion of those leaving that are working Over Time are high compared to those that are not leaving\nTraining and Education What can you deduce about the interaction between Training Times Last Year and Attrition c It’s difficult to deduce anything based on the visualization\nTime-Based Features What can you deduce about the interaction between Years At Company and Attrition b People that leave tend to have less working years at the company\nTime-Based Features What can you deduce about the interaction between Years Since Last Promotion and Attrition?\n\n\nIt’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  }
]